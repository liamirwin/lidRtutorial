[
  {
    "objectID": "01_read.html",
    "href": "01_read.html",
    "title": "Read/Plot/Query/Validate",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "01_read.html#relevant-resources",
    "href": "01_read.html#relevant-resources",
    "title": "Read/Plot/Query/Validate",
    "section": "Relevant Resources",
    "text": "Relevant Resources\nCode\nlidRbook section"
  },
  {
    "objectID": "01_read.html#overview",
    "href": "01_read.html#overview",
    "title": "Read/Plot/Query/Validate",
    "section": "Overview",
    "text": "Overview\nWelcome to this LiDAR data processing tutorial using R and the lidR library! In this tutorial, you will learn how to read, visualize, query, and validate LiDAR data.\nWeâ€™ll explore basic information like the header and tabular data and visualize point clouds using different color schemes based on attributes.\nWeâ€™ll use the select argument in readLAS() to load specific attributes and the filter argument to load only points of interest or apply transformations on-the-fly. Weâ€™ll validate the LiDAR data using the las_check function on different LiDAR data files to ensure data integrity.\nLetâ€™s get started with processing LiDAR data efficiently using lidR and R! Happy learning!"
  },
  {
    "objectID": "01_read.html#environment",
    "href": "01_read.html#environment",
    "title": "Read/Plot/Query/Validate",
    "section": "Environment",
    "text": "Environment\nWe start by loading the necessary packages, clearing our current environment, and specifying that some warnings be turned off to make our outputs clearer. We will do this for each section in the tutorial.\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.2.3"
  },
  {
    "objectID": "01_read.html#basic-usage",
    "href": "01_read.html#basic-usage",
    "title": "Read/Plot/Query/Validate",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nLoad and Inspect LiDAR Data\nLoad the LiDAR point cloud data from a LAS file using the readLAS() function. The data is stored in the las object. We can inspect the header information and attributes of the las object.\n\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\")\n#&gt; Warning: There are 127471 points flagged 'withheld'.\n\n# Inspect header information\nlas@header\n#&gt; File signature:           LASF \n#&gt; File source ID:           0 \n#&gt; Global encoding:\n#&gt;  - GPS Time Type: GPS Week Time \n#&gt;  - Synthetic Return Numbers: no \n#&gt;  - Well Know Text: CRS is GeoTIFF \n#&gt;  - Aggregate Model: false \n#&gt; Project ID - GUID:        00000000-0000-0000-0000-000000000000 \n#&gt; Version:                  1.2\n#&gt; System identifier:         \n#&gt; Generating software:      rlas R package \n#&gt; File creation d/y:        0/2013\n#&gt; header size:              227 \n#&gt; Offset to point data:     297 \n#&gt; Num. var. length record:  1 \n#&gt; Point data format:        0 \n#&gt; Point data record length: 20 \n#&gt; Num. of point records:    551117 \n#&gt; Num. of points by return: 402654 125588 21261 1571 43 \n#&gt; Scale factor X Y Z:       0.01 0.01 0.01 \n#&gt; Offset X Y Z:             2e+05 7300000 0 \n#&gt; min X Y Z:                203830 7358900 0 \n#&gt; max X Y Z:                203980 7359050 34.46 \n#&gt; Variable Length Records (VLR):\n#&gt;    Variable Length Record 1 of 1 \n#&gt;        Description: by LAStools of rapidlasso GmbH \n#&gt;        Tags:\n#&gt;           Key 3072 value 31983 \n#&gt; Extended Variable Length Records (EVLR):  void\n\n# Inspect attributes of the point cloud\nlas@data\n#&gt;                X       Y Z Intensity ReturnNumber NumberOfReturns\n#&gt;      1: 203851.6 7359049 0       285            1               1\n#&gt;      2: 203922.2 7359048 0       343            1               1\n#&gt;      3: 203942.9 7359045 0       104            2               2\n#&gt;      4: 203830.0 7359045 0       284            1               1\n#&gt;      5: 203841.2 7359047 0       290            1               1\n#&gt;     ---                                                          \n#&gt; 551113: 203902.5 7359050 0       259            2               2\n#&gt; 551114: 203907.1 7359050 0       206            1               1\n#&gt; 551115: 203956.0 7359050 0       309            1               1\n#&gt; 551116: 203962.5 7359050 0       100            2               2\n#&gt; 551117: 203972.6 7359050 0        46            2               2\n#&gt;         ScanDirectionFlag EdgeOfFlightline Classification Synthetic_flag\n#&gt;      1:                 0                0              2          FALSE\n#&gt;      2:                 0                0              2          FALSE\n#&gt;      3:                 0                0              2          FALSE\n#&gt;      4:                 0                0              2          FALSE\n#&gt;      5:                 0                0              2          FALSE\n#&gt;     ---                                                                 \n#&gt; 551113:                 0                0              2          FALSE\n#&gt; 551114:                 0                0              2          FALSE\n#&gt; 551115:                 0                0              2          FALSE\n#&gt; 551116:                 0                0              2          FALSE\n#&gt; 551117:                 0                0              2          FALSE\n#&gt;         Keypoint_flag Withheld_flag ScanAngleRank UserData PointSourceID\n#&gt;      1:         FALSE          TRUE           -21        0            14\n#&gt;      2:         FALSE          TRUE           -21        0            14\n#&gt;      3:         FALSE          TRUE           -21        0            14\n#&gt;      4:         FALSE          TRUE           -21        0            14\n#&gt;      5:         FALSE          TRUE           -21        0            14\n#&gt;     ---                                                                 \n#&gt; 551113:         FALSE          TRUE            -3        0            15\n#&gt; 551114:         FALSE          TRUE            -3        0            15\n#&gt; 551115:         FALSE          TRUE           -21        0            14\n#&gt; 551116:         FALSE          TRUE           -21        0            14\n#&gt; 551117:         FALSE          TRUE            -1        0            15\n\n# Check the file size of the loaded LiDAR data\nformat(object.size(las), \"Mb\")\n#&gt; [1] \"37.9 Mb\"\n\n\n\nVisualize LiDAR Data\nWe can visualize the LiDAR data using the plot() function. We have several options to control the colors in the plot, such as selecting specific attributes from the data to be used as colors.\n\n\n\n\n\n\nlidR plot background colour\n\n\n\nSet the background of plots to white using plot(las, bg = \"white\"). To keep the code clean Iâ€™ve omitted it from examples.\n\n\nplot(las)\n\n\n\n\n\n\n\n\n\nplot(las, color = \"Intensity\")\n\n\n\n\n\n\n\n\n\nplot(las, color = \"Classification\")\n\n\n\n\n\n\n\n\n\nplot(las, color = \"ScanAngleRank\", axis = TRUE, legend = TRUE)"
  },
  {
    "objectID": "01_read.html#optimized-usage",
    "href": "01_read.html#optimized-usage",
    "title": "Read/Plot/Query/Validate",
    "section": "Optimized Usage",
    "text": "Optimized Usage\n\nSelecting Attributes of Interest\nThe readLAS() function allows us to select specific attributes to be loaded into memory. This is useful to save memory when dealing with large LiDAR datasets.\n\n# Load only the xyz coordinates (X, Y, Z) and ignore other attributes\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\", select = \"xyz\")\n#&gt; Warning: There are 127471 points flagged 'withheld'.\n\n# Inspect the loaded attributes\nlas@data\n#&gt;                X       Y Z\n#&gt;      1: 203851.6 7359049 0\n#&gt;      2: 203922.2 7359048 0\n#&gt;      3: 203942.9 7359045 0\n#&gt;      4: 203830.0 7359045 0\n#&gt;      5: 203841.2 7359047 0\n#&gt;     ---                   \n#&gt; 551113: 203902.5 7359050 0\n#&gt; 551114: 203907.1 7359050 0\n#&gt; 551115: 203956.0 7359050 0\n#&gt; 551116: 203962.5 7359050 0\n#&gt; 551117: 203972.6 7359050 0\n\n# Check the memory size after loading only the selected attributes\nformat(object.size(las), \"Mb\")\n#&gt; [1] \"12.6 Mb\"\n\n\n\nFiltering Points of Interest\nWe can also load only a subset of the LiDAR points based on certain criteria using the filter argument in readLAS().\n\n# Load only the first return points\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\", filter = \"-keep_first\")\n#&gt; Warning: There are 93138 points flagged 'withheld'.\n\n# Inspect the loaded points\nlas\n#&gt; class        : LAS (v1.2 format 0)\n#&gt; memory       : 20 Mb \n#&gt; extent       : 203830, 203980, 7358900, 7359050 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : SIRGAS 2000 / UTM zone 23S \n#&gt; area         : 22500 mÂ²\n#&gt; points       : 402.7 thousand points\n#&gt; density      : 17.9 points/mÂ²\n#&gt; density      : 17.9 pulses/mÂ²\n\n# Check the memory size after loading only the filtered points\nformat(object.size(las), \"Mb\")\n#&gt; [1] \"27.7 Mb\"\n\nplot(las)\n\n\n\n\n\n\n\n\n\n\n\nApplying Transformation on-the-fly\nThe filter argument in readLAS() can also be used to apply a transformation to the points on-the-fly during loading. This can be useful for tasks such as normalizing the point cloud heights.\n\n# Load and visualize with an applied filter\nlas &lt;- readLAS(files = \"data/MixedEucaNat.laz\", filter = \"-keep_class 2\")\n#&gt; Warning: There are 40553 points flagged 'withheld'.\n\nplot(las)\n\n\n\n\n\n\n\n\n\n\n\nFiltering Points using filter_poi()\nAn alternative method for filtering points is using the filter_poi() function. This function allows filtering based on attributes of points.\n\n# Filter points with Classification == 2\nclass_2 &lt;- filter_poi(las = las, Classification == 2L)\n\n# Combine queries to filter points with Classification == 1 and ReturnNumber == 1\nfirst_ground &lt;- filter_poi(las = las, Classification == 2L & ReturnNumber == 1L)\n\nplot(class_2)\n\n\n\n\n\n\n\n\n\nplot(first_ground)\n\n\n\n\n\n\n\n\n\n\n\nLAS Objects Validation\nThe lidR package provides a function las_check() to validate LAS objects for common issues.\n\n# Load and validate LAS data\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\")\n#&gt; Warning: There are 127471 points flagged 'withheld'.\nlas_check(las)\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking degenerated ground points...\n#&gt;  \u001b[1;33m   âš  There were 37 degenerated ground points. Some X Y coordinates were repeated but with different Z coordinates\u001b[0m\n#&gt;   - Checking attribute population...\n#&gt;  \u001b[0;32m   ðŸ›ˆ 'ScanDirectionFlag' attribute is not populated\u001b[0m\n#&gt;  \u001b[0;32m   ðŸ›ˆ 'EdgeOfFlightline' attribute is not populated\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\n#&gt;  \u001b[0;32m   ðŸ›ˆ 127471 points flagged 'withheld'\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking negative outliers...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;32m yes\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\n#&gt;    -  ScanDirectionFlag is compressed\n#&gt;    -  EdgeOfFlightline is compressed\n#&gt;    -  Synthetic_flag is compressed\n#&gt;    -  Keypoint_flag is compressed\n#&gt;    -  UserData is compressed\n\n# Visualize corrupted LAS data\nlas &lt;- readLAS(files = \"data/example_corrupted.laz\")\n#&gt; Warning: Invalid data: 174638 points with a 'return number' greater than the\n#&gt; 'number of returns'.\n\nplot(las)\n\n\n\n\n\n\n\n\n\n\n# Validate corrupted LAS data\nlas_check(las)\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\n#&gt;  \u001b[1;33m   âš  Invalid data: 174638 points with a 'return number' greater than the 'number of returns'.\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\n#&gt;  \u001b[1;33m   âš  202348 points are duplicated and share XYZ coordinates with other points\u001b[0m\n#&gt;   - Checking degenerated ground points...\n#&gt;  \u001b[1;33m   âš  There were 31445 degenerated ground points. Some X Y Z coordinates were repeated\u001b[0m\n#&gt;   - Checking attribute population...\n#&gt;  \u001b[0;32m   ðŸ›ˆ 'PointSourceID' attribute is not populated\u001b[0m\n#&gt;  \u001b[0;32m   ðŸ›ˆ 'ScanDirectionFlag' attribute is not populated\u001b[0m\n#&gt;  \u001b[0;32m   ðŸ›ˆ 'EdgeOfFlightline' attribute is not populated\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking negative outliers...\n#&gt;  \u001b[1;33m   âš  77 points below 0\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;31m no\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\n#&gt;    -  ScanDirectionFlag is compressed\n#&gt;    -  EdgeOfFlightline is compressed\n#&gt;    -  Synthetic_flag is compressed\n#&gt;    -  Keypoint_flag is compressed\n#&gt;    -  Withheld_flag is compressed\n#&gt;    -  ScanAngleRank is compressed\n#&gt;    -  UserData is compressed\n#&gt;    -  PointSourceID is compressed"
  },
  {
    "objectID": "01_read.html#exercises-and-questions",
    "href": "01_read.html#exercises-and-questions",
    "title": "Read/Plot/Query/Validate",
    "section": "Exercises and Questions",
    "text": "Exercises and Questions\nUsing:\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\")\n\nE1.\nWhat are withheld points? Where are they in our point cloud?\n\n\nE2.\nRead the file dropping withheld points.\n\n\nE3.\nThe withheld points seem to be legitimate points that we want to keep. Try to load the file including the withheld points but get rid of the warning (without using suppressWarnings()). Hint: Check available -set_withheld filters using readLAS(filter = \"-h\").\n\n\nE4.\nLoad only the ground points and plot the point cloud colored by the return number of the point. Do it loading the strict minimal amount of memory (4.7 Mb). Hint: use ?lidR::readLAS and see what select options might help."
  },
  {
    "objectID": "01_read.html#conclusion",
    "href": "01_read.html#conclusion",
    "title": "Read/Plot/Query/Validate",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes our tutorial on the basic usage of the lidR package in R for processing and analyzing LiDAR data. We covered loading LiDAR data, inspecting and visualizing the data, selecting specific attributes, filtering points, and validating LAS objects for issues."
  },
  {
    "objectID": "02_roi.html",
    "href": "02_roi.html",
    "title": "Regions of Interest",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "02_roi.html#relevant-resources",
    "href": "02_roi.html#relevant-resources",
    "title": "Regions of Interest",
    "section": "Relevant Resources",
    "text": "Relevant Resources\nCode\nlidRbook section"
  },
  {
    "objectID": "02_roi.html#overview",
    "href": "02_roi.html#overview",
    "title": "Regions of Interest",
    "section": "Overview",
    "text": "Overview\nThis code demonstrates the selection of regions of interest from LiDAR data. Simple geometries like circles and rectangles are selected based on coordinates. Complex geometries are extracted from shapefiles to clip specific areas."
  },
  {
    "objectID": "02_roi.html#environment",
    "href": "02_roi.html#environment",
    "title": "Regions of Interest",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.2.3"
  },
  {
    "objectID": "02_roi.html#simple-geometries",
    "href": "02_roi.html#simple-geometries",
    "title": "Regions of Interest",
    "section": "Simple Geometries",
    "text": "Simple Geometries\n\nLoad LiDAR Data and Inspect\nWe start by loading the LiDAR point cloud data and inspecting its header and the number of point records.\n\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\", filter = \"-set_withheld_flag 0\")\n\n# Inspect the header and the number of point records\nlas@header\n#&gt; File signature:           LASF \n#&gt; File source ID:           0 \n#&gt; Global encoding:\n#&gt;  - GPS Time Type: GPS Week Time \n#&gt;  - Synthetic Return Numbers: no \n#&gt;  - Well Know Text: CRS is GeoTIFF \n#&gt;  - Aggregate Model: false \n#&gt; Project ID - GUID:        00000000-0000-0000-0000-000000000000 \n#&gt; Version:                  1.2\n#&gt; System identifier:         \n#&gt; Generating software:      rlas R package \n#&gt; File creation d/y:        0/2013\n#&gt; header size:              227 \n#&gt; Offset to point data:     297 \n#&gt; Num. var. length record:  1 \n#&gt; Point data format:        0 \n#&gt; Point data record length: 20 \n#&gt; Num. of point records:    551117 \n#&gt; Num. of points by return: 402654 125588 21261 1571 43 \n#&gt; Scale factor X Y Z:       0.01 0.01 0.01 \n#&gt; Offset X Y Z:             2e+05 7300000 0 \n#&gt; min X Y Z:                203830 7358900 0 \n#&gt; max X Y Z:                203980 7359050 34.46 \n#&gt; Variable Length Records (VLR):\n#&gt;    Variable Length Record 1 of 1 \n#&gt;        Description: by LAStools of rapidlasso GmbH \n#&gt;        Tags:\n#&gt;           Key 3072 value 31983 \n#&gt; Extended Variable Length Records (EVLR):  void\nlas@header$`Number of point records`\n#&gt; [1] 551117\n\n\n\nSelect Circular and Rectangular Areas\nWe can select circular and rectangular areas from the LiDAR data based on specified coordinates and radii or dimensions.\n\n# Establish coordinates\nx &lt;- 203890\ny &lt;- 7358935\n\n# Select a circular area\ncircle &lt;- clip_circle(las = las, xcenter = x, ycenter = y, radius = 30)\n\n# Inspect the circular area and the number of point records\ncircle\n#&gt; class        : LAS (v1.2 format 0)\n#&gt; memory       : 3.4 Mb \n#&gt; extent       : 203860, 203920, 7358905, 7358965 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : SIRGAS 2000 / UTM zone 23S \n#&gt; area         : 2909 mÂ²\n#&gt; points       : 74.7 thousand points\n#&gt; density      : 25.69 points/mÂ²\n#&gt; density      : 17.71 pulses/mÂ²\ncircle@header$`Number of point records`\n#&gt; [1] 74737\n\n# Plot the circular area\nplot(circle)\n\n\n\n\n\n\n\n\n\n\n# Select a rectangular area\nrect &lt;- clip_rectangle(las = las, xleft = x, ybottom = y, xright = x + 40, ytop = y + 30)\n\n# Plot the rectangular area\nplot(rect)\n\n\n\n\n\n\n\n\n\n\n# Select multiple random circular areas\nx &lt;- runif(2, x, x)\ny &lt;- runif(2, 7358900, 7359050)\n\nplots &lt;- clip_circle(las = las, xcenter = x, ycenter = y, radius = 10)\n\n# Plot each of the multiple circular areas\nplot(plots[[1]])\n\n\n\n\n\n\n\n\n\n# Plot each of the multiple circular areas\nplot(plots[[2]])"
  },
  {
    "objectID": "02_roi.html#extraction-of-complex-geometries-from-shapefiles",
    "href": "02_roi.html#extraction-of-complex-geometries-from-shapefiles",
    "title": "Regions of Interest",
    "section": "Extraction of Complex Geometries from Shapefiles",
    "text": "Extraction of Complex Geometries from Shapefiles\nIn this section, we demonstrate how to extract complex geometries from shapefiles using the clip_roi() function from the lidR package.\n\n\n\n\n\n\nLegacy packages\n\n\n\nmaptools, rgdal, and rgeos, underpinning the sp package, will retire in October 2023. Please refer to R-spatial evolution reports for details, especially https://r-spatial.org/r/2023/05/15/evolution4.html.\n\n\n\n# Load the shapefile using sf\nplanting &lt;- sf::st_read(dsn = \"data/shapefiles/MixedEucaNat.shp\", quiet = TRUE)\n\n# Plot the LiDAR header information without the map\nplot(las@header, map = FALSE)\n\n# Plot the planting areas on top of the LiDAR header plot\nplot(planting, add = TRUE, col = \"#08B5FF39\")\n\n\n\n\n\n\n\n\n# Extract points within the planting areas using clip_roi()\neucalyptus &lt;- clip_roi(las = las, geometry = planting)\n\n# Plot the extracted points within the planting areas\nplot(eucalyptus)"
  },
  {
    "objectID": "02_roi.html#exercises-and-questions",
    "href": "02_roi.html#exercises-and-questions",
    "title": "Regions of Interest",
    "section": "Exercises and Questions",
    "text": "Exercises and Questions\nNow, letâ€™s read a shapefile called MixedEucaNatPlot.shp using sf::st_read() and plot it on top of the LiDAR header plot.\n# Read the shapefile \"MixedEucaNatPlot.shp\" using st_read()\nplots &lt;- sf::st_read(dsn = \"data/shapefiles/MixedEucaNatPlot.shp\", quiet = TRUE)\n\n# Plot the LiDAR header information without the map\nplot(las@header, map = FALSE)\n\n# Plot the extracted points within the planting areas\nplot(plots, add = TRUE)\n\nE1.\nClip the 5 plots with a radius of 11.3 m.\n\n\nE2.\nClip a transect from A c(203850, 7358950) to B c(203950, 7959000).\n\n\nE3.\nClip a transect from A c(203850, 7358950) to B c(203950, 7959000) but reorient it so it is no longer on the XY diagonal. Hint = ?clip_transect"
  },
  {
    "objectID": "02_roi.html#conclusion",
    "href": "02_roi.html#conclusion",
    "title": "Regions of Interest",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes our tutorial on selecting simple geometries and extracting complex geometries from shapefiles using the lidR package in R."
  },
  {
    "objectID": "03_aba.html",
    "href": "03_aba.html",
    "title": "Area-based metrics",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "03_aba.html#relevant-resources",
    "href": "03_aba.html#relevant-resources",
    "title": "Area-based metrics",
    "section": "Relevant Resources",
    "text": "Relevant Resources\nCode\n\nlidRbook metrics section\nlidRbook modelling section"
  },
  {
    "objectID": "03_aba.html#overview",
    "href": "03_aba.html#overview",
    "title": "Area-based metrics",
    "section": "Overview",
    "text": "Overview\nThis code demonstrates an area-based approach for LiDAR data. Basic usage involves computing mean and max height of points within 10x10 m pixels and visualizing the results. The code shows how to compute multiple metrics simultaneously and use predefined metric sets. Advanced usage introduces user-defined metrics for more specialized calculations."
  },
  {
    "objectID": "03_aba.html#environment",
    "href": "03_aba.html#environment",
    "title": "Area-based metrics",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.2.3"
  },
  {
    "objectID": "03_aba.html#basic-usage",
    "href": "03_aba.html#basic-usage",
    "title": "Area-based metrics",
    "section": "Basic Usage",
    "text": "Basic Usage\nWeâ€™ll cover the basic usage of the lidR package to compute metrics from LiDAR data.\n\n# Load LiDAR data, excluding withheld flag points\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\", select = \"*\",  filter = \"-set_withheld_flag 0\")\n\n\n# Compute the mean height of points within 10x10 m pixels\nhmean &lt;- pixel_metrics(las = las, func = ~mean(Z), res = 10)\nhmean\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 15, 15, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 10, 10  (x, y)\n#&gt; extent      : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :           V1 \n#&gt; min value   :  0.001065319 \n#&gt; max value   : 17.730712824\nplot(hmean, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n# Compute the max height of points within 10x10 m pixels\nhmax &lt;- pixel_metrics(las = las, func = ~max(Z), res = 10)\nhmax\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 15, 15, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 10, 10  (x, y)\n#&gt; extent      : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   :  0.75 \n#&gt; max value   : 34.46\nplot(hmax, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n# Compute several metrics at once using a list\nmetrics &lt;- pixel_metrics(las = las, func = ~list(hmax = max(Z), hmean = mean(Z)), res = 10)\nmetrics\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 15, 15, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 10, 10  (x, y)\n#&gt; extent      : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; names       :  hmax,        hmean \n#&gt; min values  :  0.75,  0.001065319 \n#&gt; max values  : 34.46, 17.730712824\nplot(metrics, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n# Simplify computing metrics with predefined sets of metrics\nmetrics &lt;- pixel_metrics(las = las, func = .stdmetrics_z, res = 10)\nmetrics\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 15, 15, 36  (nrow, ncol, nlyr)\n#&gt; resolution  : 10, 10  (x, y)\n#&gt; extent      : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; names       :  zmax,        zmean,         zsd,     zskew,       zkurt,    zentropy, ... \n#&gt; min values  :  0.75,  0.001065319,  0.02499118, -1.860858,    1.127885, 0.005277377, ... \n#&gt; max values  : 34.46, 17.730712824, 12.95950270, 41.207184, 1738.774391, 0.955121240, ...\nplot(metrics, col = height.colors(50))\n\n\n\n\n\n\n\n\n# Plot a specific metric from the predefined set\nplot(metrics, \"zsd\", col = height.colors(50))"
  },
  {
    "objectID": "03_aba.html#advanced-usage-with-user-defined-metrics",
    "href": "03_aba.html#advanced-usage-with-user-defined-metrics",
    "title": "Area-based metrics",
    "section": "Advanced Usage with User-Defined Metrics",
    "text": "Advanced Usage with User-Defined Metrics\n\n\n\n\n\n\n3rd party metric packages\n\n\n\nlidR provides flexibility for users to define custom metrics. Check out 3rd party packages like lidRmetrics for suites of metrics.\n\n\n\n# Generate a user-defined function to compute weighted mean\nf &lt;- function(x, weight) { sum(x*weight)/sum(weight) }\n\n# Compute grid metrics for the user-defined function\nX &lt;- pixel_metrics(las = las, func = ~f(Z, Intensity), res = 10)\n\n# Visualize the output\nplot(X, col = height.colors(50))"
  },
  {
    "objectID": "03_aba.html#exercises-and-questions",
    "href": "03_aba.html#exercises-and-questions",
    "title": "Area-based metrics",
    "section": "Exercises and Questions",
    "text": "Exercises and Questions\nUsing:\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", select = \"*\",  filter = \"-set_withheld_flag 0\")\n\nE1.\nAssuming that biomass is estimated using the equation B = 0.5 * mean Z + 0.9 * 90th percentile of Z applied on first returns only, map the biomass.\n\n\nE2.\nMap the density of ground returns at a 5 m resolution with pixel_metrics(filter = ~Classification == LASGROUND).\n\n\nE3.\nMap pixels that are flat (planar) using stdshapemetrics. These could indicate potential roads."
  },
  {
    "objectID": "03_aba.html#conclusion",
    "href": "03_aba.html#conclusion",
    "title": "Area-based metrics",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we covered basic usage of the lidR package for computing mean and max heights within grid cells and using predefined sets of metrics. Additionally, we explored the advanced usage with the ability to define user-specific metrics for grid computation."
  },
  {
    "objectID": "04_chm.html",
    "href": "04_chm.html",
    "title": "Canopy Height Models",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "04_chm.html#relevant-resources",
    "href": "04_chm.html#relevant-resources",
    "title": "Canopy Height Models",
    "section": "Relevant Resources",
    "text": "Relevant Resources\nCode\nlidRbook section"
  },
  {
    "objectID": "04_chm.html#overview",
    "href": "04_chm.html#overview",
    "title": "Canopy Height Models",
    "section": "Overview",
    "text": "Overview\nThis code demonstrates the creation of a Canopy Height Model (CHM) using LiDAR data. It shows different algorithms for generating CHMs and provides options for adjusting resolution, sub-circle size, and filling empty pixels."
  },
  {
    "objectID": "04_chm.html#environment",
    "href": "04_chm.html#environment",
    "title": "Canopy Height Models",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(microbenchmark)\n#&gt; Warning: package 'microbenchmark' was built under R version 4.2.3\nlibrary(terra)\n#&gt; Warning: package 'terra' was built under R version 4.2.3"
  },
  {
    "objectID": "04_chm.html#data-preprocessing",
    "href": "04_chm.html#data-preprocessing",
    "title": "Canopy Height Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nIn this section, we load the LiDAR data, set a random fraction filter to reduce point density, and visualize the resulting LiDAR point cloud.\n\n# Load LiDAR data and reduce point density\nlas &lt;- readLAS(files = \"data/MixedEucaNat_normalized.laz\", filter = \"-keep_random_fraction 0.4 -set_withheld_flag 0\")\n\n# Visualize the LiDAR point cloud\nplot(las)"
  },
  {
    "objectID": "04_chm.html#point-to-raster-based-algorithm",
    "href": "04_chm.html#point-to-raster-based-algorithm",
    "title": "Canopy Height Models",
    "section": "Point-to-Raster Based Algorithm",
    "text": "Point-to-Raster Based Algorithm\nIn this section, we demonstrate a simple method for generating Canopy Height Models (CHMs) that assigns the elevation of the highest point to each pixel.\n\n# Generate the CHM using a simple point-to-raster based algorithm\nchm &lt;- grid_canopy(las = las, res = 2, algorithm = p2r())\n\n# Visualize the CHM\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nIn the first code chunk, we generate a Canopy Height Model (CHM) using a point-to-raster based algorithm. The grid_canopy function with the p2r() algorithm assigns the elevation of the highest point within each grid cell to the corresponding pixel. The resulting CHM is then visualized using the plot() function.\n\n# The above method is strictly equivalent to using pixel_metrics to compute max height\nchm &lt;- pixel_metrics(las = las, func = ~max(Z), res = 2)\n\n# Visualize the CHM\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe code chunk above shows that the point-to-raster based algorithm is equivalent to using pixel_metrics with a function that computes the maximum height (max(Z)) within each grid cell. The resulting CHM is visualized using the plot() function.\n\n# However, the grid_canopy algorithm is optimized\nmicrobenchmark::microbenchmark(canopy = grid_canopy(las = las, res = 1, algorithm = p2r()),\n                               metrics = pixel_metrics(las = las, func = ~max(Z), res = 1),\n                               times = 10)\n#&gt; Unit: milliseconds\n#&gt;     expr      min       lq      mean    median       uq      max neval\n#&gt;   canopy  22.1562  22.9965  28.00966  26.54225  32.3731  38.7951    10\n#&gt;  metrics 106.5774 113.7054 130.40337 122.19880 131.5295 202.8887    10\n\nThe above code chunk uses microbenchmark::microbenchmark() to compare the performance of the grid_canopy() function with p2r() algorithm and pixel_metrics() function with max(Z) for maximum height computation. It demonstrates that the grid_canopy() function is optimized for generating CHMs.\n\n# Increasing the resolution results in fewer empty pixels\nchm &lt;- grid_canopy(las = las, res = 1, algorithm = p2r())\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nBy increasing the resolution of the CHM (reducing the grid cell size), we get a more detailed representation of the canopy with fewer empty pixels.\n\n# Using the 'subcircle' option turns each point into a disc of 8 points with a radius r\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = p2r(subcircle = 0.15))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe grid_canopy() function with the p2r() algorithm allows the use of the subcircle option, which turns each LiDAR point into a disc of 8 points with a specified radius. This can help to capture more fine-grained canopy details in the resulting CHM.\n\n# Increasing the subcircle radius, but it may not have meaningful results\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = p2r(subcircle = 0.8))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nIncreasing the subcircle radius may not necessarily result in meaningful CHMs, as it could lead to over-smoothing or loss of important canopy information.\n\n# We can fill empty pixels using TIN interpolation\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = p2r(subcircle = 0.15, na.fill = tin()))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe p2r() algorithm also allows filling empty pixels using TIN (Triangulated Irregular Network) interpolation, which can help in areas with sparse LiDAR points to obtain a smoother CHM."
  },
  {
    "objectID": "04_chm.html#triangulation-based-algorithm",
    "href": "04_chm.html#triangulation-based-algorithm",
    "title": "Canopy Height Models",
    "section": "Triangulation Based Algorithm",
    "text": "Triangulation Based Algorithm\nIn this section, we demonstrate a triangulation-based algorithm for generating CHMs.\n\n# Triangulation of first returns to generate the CHM\nchm &lt;- grid_canopy(las = las, res = 1, algorithm = dsmtin())\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe grid_canopy() function with the dsmtin() algorithm generates a CHM by performing triangulation on the first returns from the LiDAR data. The resulting CHM represents the surface of the canopy.\n\n# Increasing the resolution results in a more detailed CHM\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = dsmtin())\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nIncreasing the resolution of the CHM using the res argument provides a more detailed representation of the canopy, capturing finer variations in the vegetation.\n\n# Using the Khosravipour et al. pit-free algorithm with specified thresholds and maximum edge length\nthresholds &lt;- c(0, 5, 10, 20, 25, 30)\nmax_edge &lt;- c(0, 1.35)\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = pitfree(thresholds, max_edge))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe grid_canopy function can also use the Khosravipour et al.Â pit-free algorithm with specified height thresholds and a maximum edge length to generate a CHM. This algorithm aims to correct depressions in the CHM surface.\n\n# Using the 'subcircle' option with the pit-free algorithm\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = pitfree(thresholds, max_edge, 0.1))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nThe subcircle option can be used with the pit-free algorithm to create finer CHMs with subcircles for each LiDAR point, similar to the point-to-raster based algorithm."
  },
  {
    "objectID": "04_chm.html#post-processing",
    "href": "04_chm.html#post-processing",
    "title": "Canopy Height Models",
    "section": "Post-Processing",
    "text": "Post-Processing\nUsually, CHMs can be post-processed by smoothing or other manipulations. Here, we demonstrate post-processing using the terra::focal() function for smoothing.\n\n# Post-process the CHM using the 'terra' package and focal() function for smoothing\nker &lt;- matrix(1, 3, 3)\nschm &lt;- terra::focal(chm, w = ker, fun = mean, na.rm = TRUE)\n\n# Visualize the smoothed CHM\nplot(schm, col = height.colors(50))"
  },
  {
    "objectID": "04_chm.html#conclusion",
    "href": "04_chm.html#conclusion",
    "title": "Canopy Height Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial covered different algorithms for generating Canopy Height Models (CHMs) from LiDAR data using the lidR package in R. It includes point-to-raster-based algorithms and triangulation-based algorithms, as well as post-processing using the terra package. The code chunks are well-labeled to help the audience navigate through the tutorial easily."
  },
  {
    "objectID": "05_dtm.html",
    "href": "05_dtm.html",
    "title": "Digital Terrain Models",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "05_dtm.html#relevant-resources",
    "href": "05_dtm.html#relevant-resources",
    "title": "Digital Terrain Models",
    "section": "Relevant resources",
    "text": "Relevant resources\nCode\nlidRbook section"
  },
  {
    "objectID": "05_dtm.html#overview",
    "href": "05_dtm.html#overview",
    "title": "Digital Terrain Models",
    "section": "Overview",
    "text": "Overview\nThis tutorial explores the creation of a Digital Terrain Model (DTM) from LiDAR data. It demonstrates two algorithms for DTM generation, ground point triangulation, and inverse-distance weighting. Additionally, the tutorial showcases DTM-based normalization and point-based normalization, accompanied by exercises for hands-on practice."
  },
  {
    "objectID": "05_dtm.html#environment",
    "href": "05_dtm.html#environment",
    "title": "Digital Terrain Models",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3"
  },
  {
    "objectID": "05_dtm.html#dtm-digital-terrain-model",
    "href": "05_dtm.html#dtm-digital-terrain-model",
    "title": "Digital Terrain Models",
    "section": "DTM (Digital Terrain Model)",
    "text": "DTM (Digital Terrain Model)\nIn this section, weâ€™ll generate a Digital Terrain Model (DTM) from LiDAR data using two different algorithms: tin() and knnidw().\n\nData Preprocessing\n\n# Load LiDAR data and filter out non-ground points\nlas &lt;- readLAS(files = \"data/MixedEucaNat.laz\", filter = \"-set_withheld_flag 0\")\n\nHere, we load the LiDAR data and exclude points flagged as withheld.\n\n\nVisualizing LiDAR Data\nWe start by visualizing the entire LiDAR point cloud to get an initial overview.\nplot(las)\n\n\n\n\n\n\n\n\n\nVisualizing the LiDAR data again, this time to distinguish ground points (blue) more effectively.\nplot(las, color = \"Classification\")\n\n\n\n\n\n\n\n\n\n\n\nTriangulation Algorithm - tin()\nWe create a DTM using the TIN algorithm with a specified resolution (1 meter).\n\n# Generate a DTM using the TIN (Triangulated Irregular Network) algorithm\ndtm_tin &lt;- grid_terrain(las = las, res = 1, algorithm = tin())\n#&gt; Warning: There were 37 degenerated ground points. Some X Y coordinates were\n#&gt; repeated but with different Z coordinates. min Z were retained.\n\n\n\n\n\n\n\nDegenerated points\n\n\n\nA degenerated point in LiDAR data refers to a point with identical XY(Z) coordinates as another point. This means two or more points occupy exactly the same location in XY/3D space. Degenerated points can cause issues in tasks like creating a digital terrain model, as they donâ€™t add new information and can lead to inconsistencies. Identifying and handling degenerated points appropriately is crucial for accurate and meaningful results.\n\n\n\n\nVisualizing DTM in 3D\nTo better conceptualize the terrain, we visualize the generated DTM in a 3D plot.\n# Visualize the DTM in 3D\nplot_dtm3d(dtm_tin)\n\n\n\n\n\n\n\n\n\n\n\nVisualizing DTM with LiDAR Data\nWe overlay the DTM on the LiDAR data (non-ground points only) for a more comprehensive view of the terrain.\n# Filter for non-ground points to show dtm better\nlas_ng &lt;- filter_poi(las = las, Classification != 2L)\n\n# Visualize the LiDAR data with the overlaid DTM in 3D\nx &lt;- plot(las_ng, bg = \"white\")\nadd_dtm3d(x, dtm_tin, bg = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nInverse-Distance Weighting (IDW) Algorithm - knnidw()\nNext, we generate a DTM using the IDW algorithm to compare results with the TIN-based DTM.\n\n# Generate a DTM using the IDW (Inverse-Distance Weighting) algorithm\ndtm_idw &lt;- grid_terrain(las = las, res = 1, algorithm = knnidw())\n#&gt; Warning: There were 37 degenerated ground points. Some X Y coordinates were\n#&gt; repeated but with different Z coordinates. min Z were retained.\n\n\n\nVisualizing IDW-based DTM in 3D\nWe visualize the DTM generated using the IDW algorithm in a 3D plot.\n# Visualize the IDW-based DTM in 3D\nplot_dtm3d(dtm_idw)"
  },
  {
    "objectID": "05_dtm.html#normalization",
    "href": "05_dtm.html#normalization",
    "title": "Digital Terrain Models",
    "section": "Normalization",
    "text": "Normalization\nIn this section, weâ€™ll focus on height normalization of LiDAR data using both DTM-based and point-based normalization methods.\n\nDTM-based Normalization\nWe perform DTM-based normalization on the LiDAR data using the previously generated DTM.\n\n# Normalize the LiDAR data using DTM-based normalization\nnlas_dtm &lt;- normalize_height(las = las, algorithm = dtm_tin)\n\n\n\nVisualizing Normalized LiDAR Data\nWe visualize the normalized LiDAR data, illustrating heights relative to the DTM.\n# Visualize the normalized LiDAR data\nplot(nlas_dtm)\n\n\n\n\n\n\n\n\n\n\n\nFiltering Ground Points\nWe filter the normalized data to keep only the ground points.\n\n# Filter the normalized data to retain only ground points\ngnd_dtm &lt;- filter_ground(las = nlas_dtm)\n\n\n\nVisualizing Filtered Ground Points\nWe visualize the filtered ground points, focusing on the terrain after normalization.\n# Visualize the filtered ground points\nplot(gnd_dtm)\n\n\n\n\n\n\n\n\n\n\n\nHistogram of Normalized Ground Points\nA histogram helps us understand the distribution of normalized ground pointsâ€™ height.\n\n# Plot the histogram of normalized ground points' height\nhist(gnd_dtm$Z, breaks = seq(-1.5, 1.5, 0.05))\n\n\n\n\n\n\n\n\n\n\nDTM-based Normalization with TIN Algorithm\nWe perform DTM-based normalization on the LiDAR data using the TIN algorithm.\n\n# Normalize the LiDAR data using DTM-based normalization with TIN algorithm\nnlas_tin &lt;- normalize_height(las = las, algorithm = tin())\n\n\n\nVisualizing Normalized LiDAR Data with TIN\nWe visualize the normalized LiDAR data using the TIN algorithm, showing heights relative to the DTM.\n\n# Visualize the normalized LiDAR data using the TIN algorithm\nplot(nlas_tin, bg = \"white\")\n\n\n\nFiltering Ground Points (TIN-based)\nWe filter the normalized data (TIN-based) to keep only the ground points.\n\n# Filter the normalized data (TIN-based) to retain only ground points\ngnd_tin &lt;- filter_ground(las = nlas_tin)\n\n\n\nVisualizing Filtered Ground Points (TIN-based)\nWe visualize the filtered ground points after TIN-based normalization, focusing on the terrain.\n# Visualize the filtered ground points after TIN-based normalization\nplot(gnd_tin)\n\n\n\n\n\n\n\n\n\n\n\nHistogram of Normalized Ground Points (TIN-based)\nA histogram illustrates the distribution of normalized ground pointsâ€™ height after TIN-based normalization.\n\n# Plot the histogram of normalized ground points' height after TIN-based normalization\nhist(gnd_tin$Z, breaks = seq(-1.5, 1.5, 0.05))"
  },
  {
    "objectID": "05_dtm.html#exercises",
    "href": "05_dtm.html#exercises",
    "title": "Digital Terrain Models",
    "section": "Exercises",
    "text": "Exercises\n\nE1.\nPlot and compare these two normalized point-clouds. Why do they look different? Fix that. Hint: filter.\n# Load and visualize nlas1 and nlas2\nlas1 = readLAS(\"data/MixedEucaNat.laz\", filter = \"-set_withheld_flag 0\")\nnlas1 = normalize_height(las1, tin())\nnlas2 = readLAS(\"data/MixedEucaNat_normalized.laz\", filter = \"-set_withheld_flag 0\")\nplot(nlas1)\nplot(nlas2)\n\n\nE2.\nClip a plot somewhere in MixedEucaNat.laz (the non-normalized file).\n\n\nE3.\nCompute a DTM for this plot. Which method are you choosing and why?\n\n\nE4.\nCompute a DSM (digital surface model). Hint: Look back to how you made a CHM.\n\n\nE5.\nNormalize the plot.\n\n\nE6.\nCompute a CHM.\n\n\nE7.\nCompute some metrics of interest in this plot with cloud_metrics()."
  },
  {
    "objectID": "05_dtm.html#conclusion",
    "href": "05_dtm.html#conclusion",
    "title": "Digital Terrain Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial covered the creation of Digital Terrain Models (DTMs) from LiDAR data using different algorithms and explored height normalization techniques. The exercises provided hands-on opportunities to apply these concepts, enhancing understanding and practical skills."
  },
  {
    "objectID": "06_its.html",
    "href": "06_its.html",
    "title": "Individual Tree Detection & Segmentation",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "06_its.html#relevant-resources",
    "href": "06_its.html#relevant-resources",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Relevant resources",
    "text": "Relevant resources\nCode\nlidRbook section"
  },
  {
    "objectID": "06_its.html#overview",
    "href": "06_its.html#overview",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Overview",
    "text": "Overview\nThis code demonstrates individual tree segmentation (ITS) using LiDAR data. It covers CHM-based and point-cloud-based methods for tree detection and segmentation. The code also shows how to extract metrics at the tree level and visualize them."
  },
  {
    "objectID": "06_its.html#environment",
    "href": "06_its.html#environment",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.2.3\nlibrary(terra)\n#&gt; Warning: package 'terra' was built under R version 4.2.3\n\n# Read in LiDAR file and set some color palettes\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\",  filter = \"-set_withheld_flag 0\")\ncol1 &lt;- height.colors(50)\ncol2 &lt;- pastel.colors(900)"
  },
  {
    "objectID": "06_its.html#chm-based-methods",
    "href": "06_its.html#chm-based-methods",
    "title": "Individual Tree Detection & Segmentation",
    "section": "CHM based methods",
    "text": "CHM based methods\nWe start by creating a Canopy Height Model (CHM) from the LiDAR data. The grid_canopy function generates the CHM using a specified resolution (res) and a chosen algorithm, here p2r(0.15), to compute the percentiles.\n\n# Generate CHM\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = p2r(0.15))\nplot(chm, col = col1)\n\n\n\n\n\n\n\n\nAfter building the CHM, we visualize it using a color palette (col1)."
  },
  {
    "objectID": "06_its.html#optionally-smooth-the-chm",
    "href": "06_its.html#optionally-smooth-the-chm",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Optionally smooth the CHM",
    "text": "Optionally smooth the CHM\nOptionally, we can smooth the CHM using a kernel to remove small-scale variations and enhance larger features like tree canopies.\n\n# Generate kernel and smooth chm\nkernel &lt;- matrix(1, 3, 3)\nschm &lt;- terra::focal(x = chm, w = kernel, fun = median, na.rm = TRUE)\nplot(schm, col = height.colors(30))\n\n\n\n\n\n\n\n\nHere, we smooth the CHM using a median filter with a 3x3 kernel. The smoothed CHM (schm) is visualized using a color palette to represent height values."
  },
  {
    "objectID": "06_its.html#tree-detection",
    "href": "06_its.html#tree-detection",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Tree detection",
    "text": "Tree detection\nNext, we detect tree tops using the smoothed CHM. The locate_trees function identifies tree tops based on local maxima.\n\n# Detect trees\nttops &lt;- locate_trees(las = schm, algorithm = lmf(ws = 2.5))\nttops\n#&gt; Simple feature collection with 923 features and 2 fields\n#&gt; Attribute-geometry relationships: constant (2)\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XYZ\n#&gt; Bounding box:  xmin: 203830.8 ymin: 7358901 xmax: 203979.2 ymax: 7359049\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\n#&gt; First 10 features:\n#&gt;    treeID     Z                       geometry\n#&gt; 1       1  4.53 POINT Z (203878.8 7359049 4...\n#&gt; 2       2 17.25 POINT Z (203906.2 7359049 1...\n#&gt; 3       3 11.77 POINT Z (203936.2 7359049 1...\n#&gt; 4       4 21.33 POINT Z (203951.8 7359049 2...\n#&gt; 5       5 20.75 POINT Z (203977.2 7359049 2...\n#&gt; 6       6 19.22 POINT Z (203914.8 7359049 1...\n#&gt; 7       7 14.76 POINT Z (203918.8 7359048 1...\n#&gt; 8       8  7.92 POINT Z (203867.8 7359048 7...\n#&gt; 9       9  9.31 POINT Z (203897.2 7359047 9...\n#&gt; 10     10 23.53 POINT Z (203924.2 7359047 2...\nplot(chm, col = col1)\nplot(ttops, col = \"black\", add = TRUE, cex = 0.5)\n\n\n\n\n\n\n\n\nThe detected tree tops (ttops) are plotted on top of the CHM (chm) to visualize their positions."
  },
  {
    "objectID": "06_its.html#segmentation",
    "href": "06_its.html#segmentation",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Segmentation",
    "text": "Segmentation\nNow, we perform tree segmentation using the detected tree tops. The segment_trees function segments the trees in the LiDAR point cloud based on the previously detected tree tops.\n# Segment trees using dalponte\nlas &lt;- segment_trees(las = las, algorithm = dalponte2016(chm = schm, treetops = ttops))\n\n# Count number of trees detected and segmented\nlength(unique(las$treeID) |&gt; na.omit())\n#&gt; [1] 923\n\n# Visualize all trees\nplot(las, color = \"treeID\")\n\n\n\n\n\n\n\n\n\n\n# Select trees by ID\ntree25 &lt;- filter_poi(las = las, treeID == 25)\ntree125 &lt;- filter_poi(las = las, treeID == 125)\n\nplot(tree25, size = 4)\n\n\n\n\n\n\n\n\n\nplot(tree125, size = 3)\n\n\n\n\n\n\n\n\n\nAfter segmentation, we count the number of trees detected and visualize all the trees in the point cloud. We then select two trees (tree25 and tree125) and visualize them individually.\n\n\n\n\n\n\nVariability and testing\n\n\n\nForests are highly variable! This means that some algorithms and parameters will work better than others depending on the data you have. Play around with algorithms and see which works best for your data."
  },
  {
    "objectID": "06_its.html#working-with-rasters",
    "href": "06_its.html#working-with-rasters",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Working with rasters",
    "text": "Working with rasters\nThe lidR package is designed for point clouds, but some functions can be applied to raster data as well. Here, we show how to extract trees from the CHM without using the point cloud directly.\n\n# Generate rasterized delineation\ntrees &lt;- dalponte2016(chm = chm, treetops = ttops)() # Notice the parenthesis at the end\ntrees\n#&gt; class      : RasterLayer \n#&gt; dimensions : 300, 300, 90000  (nrow, ncol, ncell)\n#&gt; resolution : 0.5, 0.5  (x, y)\n#&gt; extent     : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; crs        : +proj=utm +zone=23 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n#&gt; source     : memory\n#&gt; names      : Z \n#&gt; values     : 1, 923  (min, max)\n\nplot(trees, col = col2)\nplot(ttops, add = TRUE, cex = 0.5)\n#&gt; Warning in plot.sf(ttops, add = TRUE, cex = 0.5): ignoring all but the first\n#&gt; attribute\n\n\n\n\n\n\n\n\nWe create tree objects (trees) using the dalponte2016 algorithm with the CHM and tree tops. The resulting objects are visualized alongside the detected tree tops."
  },
  {
    "objectID": "06_its.html#tree-detection-1",
    "href": "06_its.html#tree-detection-1",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Tree detection",
    "text": "Tree detection\nWe begin with tree detection using the local maxima filtering (lmf) algorithm. This approach directly works with the LiDAR point cloud to detect tree tops.\n\n# Detect trees\nttops &lt;- locate_trees(las = las, algorithm = lmf(ws = 3, hmin = 5))\n\n# Visualize\nx &lt;- plot(las)\nadd_treetops3d(x = x, ttops = ttops, radius = 0.5)\n\n\n\n\n\n\n\n\n\nWe detect tree tops using the lmf algorithm and visualize them in 3D by adding the tree tops to the LiDAR plot."
  },
  {
    "objectID": "06_its.html#tree-segmentation",
    "href": "06_its.html#tree-segmentation",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Tree segmentation",
    "text": "Tree segmentation\nNext, we perform tree segmentation using the li2012 algorithm, which directly processes the LiDAR point cloud.\n# Segment using li\nlas &lt;- segment_trees(las = las, algorithm = li2012())\nplot(las, color = \"treeID\")\n# This algorithm does not seem pertinent for this dataset.\n\n\n\n\n\n\n\n\n\nThe li2012 algorithm segments the trees in the LiDAR point cloud based on local neighborhood information. However, it may not be optimal for this specific dataset."
  },
  {
    "objectID": "06_its.html#using-crown_metrics",
    "href": "06_its.html#using-crown_metrics",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Using crown_metrics()",
    "text": "Using crown_metrics()\nThe crown_metrics() function extracts metrics from the segmented trees using a user-defined function. We use the length of the Z coordinate to obtain the tree height as an example.\n\n# Generate metrics for each delineated crown\nmetrics &lt;- crown_metrics(las = las, func = ~list(n = length(Z)))\nmetrics\n#&gt; Simple feature collection with 752 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XYZ\n#&gt; Bounding box:  xmin: 203830 ymin: 7358900 xmax: 203980 ymax: 7359050\n#&gt; z_range:       zmin: 2.02 zmax: 34.46\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\n#&gt; First 10 features:\n#&gt;    treeID    n                       geometry\n#&gt; 1       1 2774 POINT Z (203969.4 7359045 3...\n#&gt; 2       2 1918 POINT Z (203969.5 7358922 3...\n#&gt; 3       3  859 POINT Z (203967.8 7358926 3...\n#&gt; 4       4 1605 POINT Z (203943.1 7358936 3...\n#&gt; 5       5  454 POINT Z (203954.5 7358949 3...\n#&gt; 6       6  417 POINT Z (203957.8 7358949 3...\n#&gt; 7       7  946 POINT Z (203949.7 7358943 3...\n#&gt; 8       8 1671 POINT Z (203970 7358900 32.09)\n#&gt; 9       9 1106 POINT Z (203975.2 7358915 3...\n#&gt; 10     10  411 POINT Z (203947.7 7358949 3...\nplot(metrics[\"n\"], cex = 0.8)\n\n\n\n\n\n\n\n\nWe calculate the number of points (n) in each tree crown using a user-defined function, and then visualize the results."
  },
  {
    "objectID": "06_its.html#applying-user-defined-functions",
    "href": "06_its.html#applying-user-defined-functions",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Applying user-defined functions",
    "text": "Applying user-defined functions\nWe can map any user-defined function at the tree level using the crown_metrics() function, just like pixel_metrics(). Here, we calculate the convex hull area of each tree using a custom function f() and then visualize the results.\n\n# User defined function for area calculation\nf &lt;- function(x, y) {\n  # Get xy for tree\n  coords &lt;- cbind(x, y)\n  \n  # Convex hull\n  ch &lt;- chull(coords)\n  \n  # Close coordinates\n  ch &lt;- c(ch, ch[1])\n  ch_coords &lt;- coords[ch, ]\n  \n  # Generate polygon\n  p &lt;- st_polygon(list(ch_coords))\n  \n  #calculate area\n  area &lt;- st_area(p)\n\n  return(list(A = area))\n}\n\n# Apply user-defined function\nmetrics &lt;- crown_metrics(las = las, func = ~f(X, Y))\nmetrics\n#&gt; Simple feature collection with 752 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XYZ\n#&gt; Bounding box:  xmin: 203830 ymin: 7358900 xmax: 203980 ymax: 7359050\n#&gt; z_range:       zmin: 2.02 zmax: 34.46\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\n#&gt; First 10 features:\n#&gt;    treeID         A                       geometry\n#&gt; 1       1 172.51720 POINT Z (203969.4 7359045 3...\n#&gt; 2       2 103.84100 POINT Z (203969.5 7358922 3...\n#&gt; 3       3  72.18270 POINT Z (203967.8 7358926 3...\n#&gt; 4       4  79.85055 POINT Z (203943.1 7358936 3...\n#&gt; 5       5  17.34630 POINT Z (203954.5 7358949 3...\n#&gt; 6       6  17.45860 POINT Z (203957.8 7358949 3...\n#&gt; 7       7  41.13120 POINT Z (203949.7 7358943 3...\n#&gt; 8       8  71.97225 POINT Z (203970 7358900 32.09)\n#&gt; 9       9  50.51530 POINT Z (203975.2 7358915 3...\n#&gt; 10     10  24.61015 POINT Z (203947.7 7358949 3...\nplot(metrics[\"A\"], cex = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3rd party metric packages\n\n\n\nRemember that you can use 3rd party packages like lidRmetrics for crown metrics too!"
  },
  {
    "objectID": "06_its.html#using-pre-defined-metrics",
    "href": "06_its.html#using-pre-defined-metrics",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Using pre-defined metrics",
    "text": "Using pre-defined metrics\nSome metrics are already recorded, and we can directly calculate them at the tree level using crown_metrics().\n\nmetrics &lt;- crown_metrics(las = las, func = .stdtreemetrics)\nmetrics\n#&gt; Simple feature collection with 752 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XYZ\n#&gt; Bounding box:  xmin: 203830 ymin: 7358900 xmax: 203980 ymax: 7359050\n#&gt; z_range:       zmin: 2.02 zmax: 34.46\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\n#&gt; First 10 features:\n#&gt;    treeID     Z npoints convhull_area                       geometry\n#&gt; 1       1 34.46    2774       172.517 POINT Z (203969.4 7359045 3...\n#&gt; 2       2 32.52    1918       103.841 POINT Z (203969.5 7358922 3...\n#&gt; 3       3 32.46     859        72.183 POINT Z (203967.8 7358926 3...\n#&gt; 4       4 32.35    1605        79.851 POINT Z (203943.1 7358936 3...\n#&gt; 5       5 32.33     454        17.346 POINT Z (203954.5 7358949 3...\n#&gt; 6       6 32.22     417        17.459 POINT Z (203957.8 7358949 3...\n#&gt; 7       7 32.14     946        41.131 POINT Z (203949.7 7358943 3...\n#&gt; 8       8 32.09    1671        71.972 POINT Z (203970 7358900 32.09)\n#&gt; 9       9 32.08    1106        50.515 POINT Z (203975.2 7358915 3...\n#&gt; 10     10 32.01     411        24.610 POINT Z (203947.7 7358949 3...\n\n# Visualize individual metrics\nplot(x = metrics[\"convhull_area\"], cex = 0.8)\n\n\n\n\n\n\n\nplot(x = metrics[\"Z\"], cex = 0.8)\n\n\n\n\n\n\n\n\nWe calculate tree-level metrics using the .stdtreemetrics function and visualize individual metrics like convex hull area and height."
  },
  {
    "objectID": "06_its.html#delineating-crowns",
    "href": "06_its.html#delineating-crowns",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Delineating crowns",
    "text": "Delineating crowns\nThe delineate_crowns() function segments trees and extracts metrics at the crown level.\n\ncvx_hulls &lt;- delineate_crowns(las = las, func = .stdtreemetrics)\ncvx_hulls\n#&gt; class       : SpatialPolygonsDataFrame \n#&gt; features    : 752 \n#&gt; extent      : 203830, 203980, 7358900, 7359050  (xmin, xmax, ymin, ymax)\n#&gt; crs         : +proj=utm +zone=23 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n#&gt; variables   : 7\n#&gt; names       : treeID,     Z, npoints, convhull_area,      XTOP,       YTOP,  ZTOP \n#&gt; min values  :      1,  2.02,       8,         0.086, 203830.02, 7358900.01,  2.02 \n#&gt; max values  :    752, 34.46,    4991,       213.816, 203979.99, 7359049.88, 34.46\n\nplot(cvx_hulls)\nplot(ttops, add = TRUE, cex = 0.5)\n#&gt; Warning in plot.sf(ttops, add = TRUE, cex = 0.5): ignoring all but the first\n#&gt; attribute\n\n\n\n\n\n\n\n\n# Visualize individual metrics based on values\nplot(x = cvx_hulls[\"convhull_area\"])\n\n\n\n\n\n\n\nplot(x = cvx_hulls[\"Z\"])\n\nWe use delineate_crowns() with the .stdtreemetrics function to segment trees and extract metrics based on crown delineation."
  },
  {
    "objectID": "06_its.html#exercises",
    "href": "06_its.html#exercises",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "06_its.html#conclusion",
    "href": "06_its.html#conclusion",
    "title": "Individual Tree Detection & Segmentation",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes the tutorial on various methods for tree detection, segmentation, and extraction of metrics using the lidR package in R."
  },
  {
    "objectID": "07_engine.html",
    "href": "07_engine.html",
    "title": "LAScatalog",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "07_engine.html#relevant-resources",
    "href": "07_engine.html#relevant-resources",
    "title": "LAScatalog",
    "section": "Relevant resources:",
    "text": "Relevant resources:\nCode\nlidRbook section"
  },
  {
    "objectID": "07_engine.html#overview",
    "href": "07_engine.html#overview",
    "title": "LAScatalog",
    "section": "Overview",
    "text": "Overview\nThis code performs various operations on LiDAR data using LAScatalog functionality. We visualize and inspect the data, validate the files, clip the data based on specific coordinates, and generate a Canopy Height Model (CHM), compute Above Ground Biomass (ABA) output, detect treetops, specify processing options, and use parallel computing."
  },
  {
    "objectID": "07_engine.html#environment",
    "href": "07_engine.html#environment",
    "title": "LAScatalog",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.2.3"
  },
  {
    "objectID": "07_engine.html#basic-usage",
    "href": "07_engine.html#basic-usage",
    "title": "LAScatalog",
    "section": "Basic Usage",
    "text": "Basic Usage\nIn this section, we will cover the basic usage of the lidR package, including reading LiDAR data, visualization, and inspecting metadata.\n\nRead catalog from directory of files\nWe begin by creating a LAS catalog (ctg) from a folder containing multiple LAS files using the readLAScatalog function.\n\n# Read catalog and drop withheld\nctg &lt;- readLAScatalog(folder = \"data/Farm_A/\",filter = \"-drop_withheld\")\n\n\n\nInspect catalog\nWe can inspect the contents of the catalog using standard R functions.\n\nctg\n#&gt; class       : LAScatalog (v1.2 format 0)\n#&gt; extent      : 207340, 208040, 7357280, 7357980 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S \n#&gt; area        : 489930 mÂ²\n#&gt; points      : 14.49 million points\n#&gt; density     : 29.6 points/mÂ²\n#&gt; density     : 23.2 pulses/mÂ²\n#&gt; num. files  : 25\n\n\n\nVisualize catalog\nWe visualize the catalog, showing the spatial coverage of the LiDAR data header extents. The map can be interactive if we use map = TRUE. Try clicking on a tile to see its header information.\n\nplot(ctg)\n\n\n\n\n\n\n\n\n# Interactive\nplot(ctg, map = TRUE)\n\n\n\n\n\n\n\n\nCheck coordinate system and extent info\nWe examine the coordinate system and extent information of the catalog.\n\n# coordinate system\ncrs(ctg)\n#&gt; Coordinate Reference System:\n#&gt; Deprecated Proj.4 representation:\n#&gt;  +proj=utm +zone=23 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m\n#&gt; +no_defs \n#&gt; WKT2 2019 representation:\n#&gt; PROJCRS[\"SIRGAS 2000 / UTM zone 23S\",\n#&gt;     BASEGEOGCRS[\"SIRGAS 2000\",\n#&gt;         DATUM[\"Sistema de Referencia Geocentrico para las AmericaS 2000\",\n#&gt;             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4674]],\n#&gt;     CONVERSION[\"UTM zone 23S\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-45,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",500000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",10000000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"Brazil - between 48Â°W and 42Â°W, northern and southern hemispheres, onshore and offshore.\"],\n#&gt;         BBOX[-33.5,-48,5.13,-42]],\n#&gt;     ID[\"EPSG\",31983]]\nprojection(ctg)\n#&gt; [1] \"+proj=utm +zone=23 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"\nst_crs(ctg)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:31983 \n#&gt;   wkt:\n#&gt; PROJCRS[\"SIRGAS 2000 / UTM zone 23S\",\n#&gt;     BASEGEOGCRS[\"SIRGAS 2000\",\n#&gt;         DATUM[\"Sistema de Referencia Geocentrico para las AmericaS 2000\",\n#&gt;             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4674]],\n#&gt;     CONVERSION[\"UTM zone 23S\",\n#&gt;         METHOD[\"Transverse Mercator\",\n#&gt;             ID[\"EPSG\",9807]],\n#&gt;         PARAMETER[\"Latitude of natural origin\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8801]],\n#&gt;         PARAMETER[\"Longitude of natural origin\",-45,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8802]],\n#&gt;         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#&gt;             SCALEUNIT[\"unity\",1],\n#&gt;             ID[\"EPSG\",8805]],\n#&gt;         PARAMETER[\"False easting\",500000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8806]],\n#&gt;         PARAMETER[\"False northing\",10000000,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8807]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"(E)\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         AXIS[\"(N)\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Engineering survey, topographic mapping.\"],\n#&gt;         AREA[\"Brazil - between 48Â°W and 42Â°W, northern and southern hemispheres, onshore and offshore.\"],\n#&gt;         BBOX[-33.5,-48,5.13,-42]],\n#&gt;     ID[\"EPSG\",31983]]\n\n# spatial extents\nextent(ctg)\n#&gt; class      : Extent \n#&gt; xmin       : 207340 \n#&gt; xmax       : 208040 \n#&gt; ymin       : 7357280 \n#&gt; ymax       : 7357980\nbbox(ctg)\n#&gt;         [,1]    [,2]\n#&gt; [1,]  207340  208040\n#&gt; [2,] 7357280 7357980\nst_bbox(ctg)\n#&gt;    xmin    ymin    xmax    ymax \n#&gt;  207340 7357280  208040 7357980"
  },
  {
    "objectID": "07_engine.html#validate-files-in-catalog",
    "href": "07_engine.html#validate-files-in-catalog",
    "title": "LAScatalog",
    "section": "Validate files in catalog",
    "text": "Validate files in catalog\nWe validate the LAS files within the catalog using the las_check function. It works the same way as it would on a regular LAS file.\n\nlas_check(las = ctg)\n#&gt; \n#&gt;  Checking headers consistency\n#&gt;   - Checking file version consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking offset consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point type consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking VLR consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking CRS consistency...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking the headers\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking Point Data Format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking negative outliers...\n#&gt;  \u001b[1;33m   âš  25 file(s) with points below 0\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;31m no\u001b[0m\n#&gt;  Checking the geometry\n#&gt;   - Checking overlapping tiles...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point indexation...\u001b[0;32m yes\u001b[0m"
  },
  {
    "objectID": "07_engine.html#file-indexing",
    "href": "07_engine.html#file-indexing",
    "title": "LAScatalog",
    "section": "File indexing",
    "text": "File indexing\nWe explore indexing of LAScatalog objects for efficient processing. The lidR policy has always been: use LAStools and lasindex for spatial indexing. If you really donâ€™t want, or canâ€™t use LAStools, then there is a hidden function in lidR that users can use (lidR:::catalog_laxindex()).\n\n# check if files have .lax\nis.indexed(ctg)\n#&gt; [1] FALSE\n\n# generate index files\nlidR:::catalog_laxindex(ctg)\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\n\n# check if files have .lax\nis.indexed(ctg)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "07_engine.html#generate-chm",
    "href": "07_engine.html#generate-chm",
    "title": "LAScatalog",
    "section": "Generate CHM",
    "text": "Generate CHM\nWe create a CHM by rasterizing the point cloud data from the catalog.\n\n# Generate CHM\nchm &lt;- rasterize_canopy(las = ctg, res = 0.5, algorithm = p2r(subcircle = 0.15))\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nWe encounter issues and warnings while generating the CHM. Letâ€™s figure out how to fix the warnings and get decent outputs.\n\n# Check for warnings\nwarnings()"
  },
  {
    "objectID": "07_engine.html#catalog-processing-options",
    "href": "07_engine.html#catalog-processing-options",
    "title": "LAScatalog",
    "section": "Catalog processing options",
    "text": "Catalog processing options\nWe explore and manipulate catalog options.\n\n# Setting options and re-rasterizing the CHM\nopt_filter(ctg) &lt;- \"-drop_z_below 0 -drop_z_above 40\"\nopt_select(ctg) &lt;- \"xyz\"\nchm &lt;- rasterize_canopy(las = ctg, res = 0.5, algorithm = p2r(subcircle = 0.15))\n#&gt; Warning: There are 176895 points flagged 'withheld'.\n#&gt; Chunk 1 of 25 (4%): state âš \n#&gt; Warning: There are 181480 points flagged 'withheld'.\n#&gt; Chunk 2 of 25 (8%): state âš \n#&gt; Warning: There are 180240 points flagged 'withheld'.\n#&gt; Chunk 3 of 25 (12%): state âš \n#&gt; Warning: There are 183999 points flagged 'withheld'.\n#&gt; Chunk 4 of 25 (16%): state âš \n#&gt; Warning: There are 180695 points flagged 'withheld'.\n#&gt; Chunk 5 of 25 (20%): state âš \n#&gt; Warning: There are 160127 points flagged 'withheld'.\n#&gt; Chunk 6 of 25 (24%): state âš \n#&gt; Warning: There are 193741 points flagged 'withheld'.\n#&gt; Chunk 7 of 25 (28%): state âš \n#&gt; Warning: There are 199696 points flagged 'withheld'.\n#&gt; Chunk 8 of 25 (32%): state âš \n#&gt; Warning: There are 185983 points flagged 'withheld'.\n#&gt; Chunk 9 of 25 (36%): state âš \n#&gt; Warning: There are 179860 points flagged 'withheld'.\n#&gt; Chunk 10 of 25 (40%): state âš \n#&gt; Warning: There are 130098 points flagged 'withheld'.\n#&gt; Chunk 11 of 25 (44%): state âš \n#&gt; Warning: There are 148560 points flagged 'withheld'.\n#&gt; Chunk 12 of 25 (48%): state âš \n#&gt; Warning: There are 154984 points flagged 'withheld'.\n#&gt; Chunk 13 of 25 (52%): state âš \n#&gt; Warning: There are 151367 points flagged 'withheld'.\n#&gt; Chunk 14 of 25 (56%): state âš \n#&gt; Warning: There are 148949 points flagged 'withheld'.\n#&gt; Chunk 15 of 25 (60%): state âš \n#&gt; Warning: There are 109875 points flagged 'withheld'.\n#&gt; Chunk 16 of 25 (64%): state âš \n#&gt; Warning: There are 139859 points flagged 'withheld'.\n#&gt; Chunk 17 of 25 (68%): state âš \n#&gt; Warning: There are 143824 points flagged 'withheld'.\n#&gt; Chunk 18 of 25 (72%): state âš \n#&gt; Warning: There are 141074 points flagged 'withheld'.\n#&gt; Chunk 19 of 25 (76%): state âš \n#&gt; Warning: There are 117537 points flagged 'withheld'.\n#&gt; Chunk 20 of 25 (80%): state âš \n#&gt; Warning: There are 112307 points flagged 'withheld'.\n#&gt; Chunk 21 of 25 (84%): state âš \n#&gt; Warning: There are 138902 points flagged 'withheld'.\n#&gt; Chunk 22 of 25 (88%): state âš \n#&gt; Warning: There are 127818 points flagged 'withheld'.\n#&gt; Chunk 23 of 25 (92%): state âš \n#&gt; Warning: There are 109880 points flagged 'withheld'.\n#&gt; Chunk 24 of 25 (96%): state âš \n#&gt; Warning: There are 105073 points flagged 'withheld'.\n\n\n\n\n\n\n\n#&gt; Chunk 25 of 25 (100%): state âš \nplot(chm, col = height.colors(50))"
  },
  {
    "objectID": "07_engine.html#area-based-approach-on-catalog",
    "href": "07_engine.html#area-based-approach-on-catalog",
    "title": "LAScatalog",
    "section": "Area-based approach on catalog",
    "text": "Area-based approach on catalog\nIn this section, we generate Above Ground Biomass (ABA) estimates using the LAScatalog.\n\nGenerate ABA output and visualize\nWe calculate ABA using the pixel_metrics function and visualize the results.\n\n# Generate area-based metrics\nmodel &lt;- pixel_metrics(las = ctg, func = ~max(Z), res = 20)\n#&gt; Warning: There are 176895 points flagged 'withheld'.\n#&gt; Chunk 1 of 25 (4%): state âš \n#&gt; Warning: There are 181480 points flagged 'withheld'.\n#&gt; Chunk 2 of 25 (8%): state âš \n#&gt; Warning: There are 180240 points flagged 'withheld'.\n#&gt; Chunk 3 of 25 (12%): state âš \n#&gt; Warning: There are 183999 points flagged 'withheld'.\n#&gt; Chunk 4 of 25 (16%): state âš \n#&gt; Warning: There are 180695 points flagged 'withheld'.\n#&gt; Chunk 5 of 25 (20%): state âš \n#&gt; Warning: There are 160127 points flagged 'withheld'.\n#&gt; Chunk 6 of 25 (24%): state âš \n#&gt; Warning: There are 193741 points flagged 'withheld'.\n#&gt; Chunk 7 of 25 (28%): state âš \n#&gt; Warning: There are 199696 points flagged 'withheld'.\n#&gt; Chunk 8 of 25 (32%): state âš \n#&gt; Warning: There are 185983 points flagged 'withheld'.\n#&gt; Chunk 9 of 25 (36%): state âš \n#&gt; Warning: There are 179860 points flagged 'withheld'.\n#&gt; Chunk 10 of 25 (40%): state âš \n#&gt; Warning: There are 130098 points flagged 'withheld'.\n#&gt; Chunk 11 of 25 (44%): state âš \n#&gt; Warning: There are 148560 points flagged 'withheld'.\n#&gt; Chunk 12 of 25 (48%): state âš \n#&gt; Warning: There are 154984 points flagged 'withheld'.\n#&gt; Chunk 13 of 25 (52%): state âš \n#&gt; Warning: There are 151367 points flagged 'withheld'.\n#&gt; Chunk 14 of 25 (56%): state âš \n#&gt; Warning: There are 148949 points flagged 'withheld'.\n#&gt; Chunk 15 of 25 (60%): state âš \n#&gt; Warning: There are 109875 points flagged 'withheld'.\n#&gt; Chunk 16 of 25 (64%): state âš \n#&gt; Warning: There are 139859 points flagged 'withheld'.\n#&gt; Chunk 17 of 25 (68%): state âš \n#&gt; Warning: There are 143824 points flagged 'withheld'.\n#&gt; Chunk 18 of 25 (72%): state âš \n#&gt; Warning: There are 141074 points flagged 'withheld'.\n#&gt; Chunk 19 of 25 (76%): state âš \n#&gt; Warning: There are 117537 points flagged 'withheld'.\n#&gt; Chunk 20 of 25 (80%): state âš \n#&gt; Warning: There are 112307 points flagged 'withheld'.\n#&gt; Chunk 21 of 25 (84%): state âš \n#&gt; Warning: There are 138902 points flagged 'withheld'.\n#&gt; Chunk 22 of 25 (88%): state âš \n#&gt; Warning: There are 127818 points flagged 'withheld'.\n#&gt; Chunk 23 of 25 (92%): state âš \n#&gt; Warning: There are 109880 points flagged 'withheld'.\n#&gt; Chunk 24 of 25 (96%): state âš \n#&gt; Warning: There are 105073 points flagged 'withheld'.\n\n\n\n\n\n\n\n#&gt; Chunk 25 of 25 (100%): state âš \nplot(model, col = height.colors(50))"
  },
  {
    "objectID": "07_engine.html#first-returns-only",
    "href": "07_engine.html#first-returns-only",
    "title": "LAScatalog",
    "section": "First returns only",
    "text": "First returns only\nWe adjust the catalog options to calculate ABA based on first returns only.\n\nopt_filter(ctg) &lt;- \"-drop_z_below 0 -drop_z_above 40 -keep_first\"\nmodel &lt;- pixel_metrics(las = ctg, func = ~max(Z), res = 20)\n#&gt; Warning: There are 136085 points flagged 'withheld'.\n#&gt; Chunk 1 of 25 (4%): state âš \n#&gt; Warning: There are 133757 points flagged 'withheld'.\n#&gt; Chunk 2 of 25 (8%): state âš \n#&gt; Warning: There are 139218 points flagged 'withheld'.\n#&gt; Chunk 3 of 25 (12%): state âš \n#&gt; Warning: There are 139749 points flagged 'withheld'.\n#&gt; Chunk 4 of 25 (16%): state âš \n#&gt; Warning: There are 137216 points flagged 'withheld'.\n#&gt; Chunk 5 of 25 (20%): state âš \n#&gt; Warning: There are 118302 points flagged 'withheld'.\n#&gt; Chunk 6 of 25 (24%): state âš \n#&gt; Warning: There are 144423 points flagged 'withheld'.\n#&gt; Chunk 7 of 25 (28%): state âš \n#&gt; Warning: There are 150632 points flagged 'withheld'.\n#&gt; Chunk 8 of 25 (32%): state âš \n#&gt; Warning: There are 146331 points flagged 'withheld'.\n#&gt; Chunk 9 of 25 (36%): state âš \n#&gt; Warning: There are 138662 points flagged 'withheld'.\n#&gt; Chunk 10 of 25 (40%): state âš \n#&gt; Warning: There are 102298 points flagged 'withheld'.\n#&gt; Chunk 11 of 25 (44%): state âš \n#&gt; Warning: There are 111254 points flagged 'withheld'.\n#&gt; Chunk 12 of 25 (48%): state âš \n#&gt; Warning: There are 114956 points flagged 'withheld'.\n#&gt; Chunk 13 of 25 (52%): state âš \n#&gt; Warning: There are 115248 points flagged 'withheld'.\n#&gt; Chunk 14 of 25 (56%): state âš \n#&gt; Warning: There are 112068 points flagged 'withheld'.\n#&gt; Chunk 15 of 25 (60%): state âš \n#&gt; Warning: There are 92112 points flagged 'withheld'.\n#&gt; Chunk 16 of 25 (64%): state âš \n#&gt; Warning: There are 105339 points flagged 'withheld'.\n#&gt; Chunk 17 of 25 (68%): state âš \n#&gt; Warning: There are 108368 points flagged 'withheld'.\n#&gt; Chunk 18 of 25 (72%): state âš \n#&gt; Warning: There are 109413 points flagged 'withheld'.\n#&gt; Chunk 19 of 25 (76%): state âš \n#&gt; Warning: There are 103329 points flagged 'withheld'.\n#&gt; Chunk 20 of 25 (80%): state âš \n#&gt; Warning: There are 90720 points flagged 'withheld'.\n#&gt; Chunk 21 of 25 (84%): state âš \n#&gt; Warning: There are 107556 points flagged 'withheld'.\n#&gt; Chunk 22 of 25 (88%): state âš \n#&gt; Warning: There are 109418 points flagged 'withheld'.\n#&gt; Chunk 23 of 25 (92%): state âš \n#&gt; Warning: There are 105598 points flagged 'withheld'.\n#&gt; Chunk 24 of 25 (96%): state âš \n#&gt; Warning: There are 104440 points flagged 'withheld'.\n\n\n\n\n\n\n\n#&gt; Chunk 25 of 25 (100%): state âš \nplot(model, col = height.colors(50))"
  },
  {
    "objectID": "07_engine.html#clip-a-catalog",
    "href": "07_engine.html#clip-a-catalog",
    "title": "LAScatalog",
    "section": "Clip a catalog",
    "text": "Clip a catalog\nWe clip the LAS data in the catalog using specified coordinate groups.\n\n# Set coordinate groups\nx &lt;- c(207846, 208131, 208010, 207852, 207400)\ny &lt;- c(7357315, 7357537, 7357372, 7357548, 7357900)\n\n# Visualize coordinate groups\nplot(ctg)\npoints(x, y)\n\n\n\n\n\n\n\n\n# Clip plots\nrois &lt;- clip_circle(las = ctg, xcenter = x, ycenter = y, radius = 30)\n#&gt; Chunk 1 of 5 (20%): state âœ“\n#&gt; Chunk 2 of 5 (40%): state âˆ…\n#&gt; Chunk 3 of 5 (60%): state âœ“\n#&gt; Chunk 4 of 5 (80%): state âœ“\n#&gt; Chunk 5 of 5 (100%): state âœ“\n#&gt; No point found for within region of interest 2.\n\n\n\n\n\n\n\n\nplot(rois[[1]])\n\n\n\n\n\n\n\n\n\nplot(rois[[3]])"
  },
  {
    "objectID": "07_engine.html#validate-clipped-data",
    "href": "07_engine.html#validate-clipped-data",
    "title": "LAScatalog",
    "section": "Validate clipped data",
    "text": "Validate clipped data\nWe validate the clipped LAS data using the las_check function.\n\nlas_check(rois[[1]])\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\n#&gt;  \u001b[1;33m   âš  10212 points are duplicated and share XYZ coordinates with other points\u001b[0m\n#&gt;   - Checking degenerated ground points...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking attribute population...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking negative outliers...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\u001b[0;31m no\u001b[0m\nlas_check(rois[[3]])\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\n#&gt;  \u001b[1;33m   âš  30645 points are duplicated and share XYZ coordinates with other points\u001b[0m\n#&gt;   - Checking degenerated ground points...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking attribute population...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;31m no\u001b[0m\n#&gt;   - Checking negative outliers...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\u001b[0;31m no\u001b[0m"
  },
  {
    "objectID": "07_engine.html#independent-files-e.g.-plots-as-catalogs",
    "href": "07_engine.html#independent-files-e.g.-plots-as-catalogs",
    "title": "LAScatalog",
    "section": "Independent files (e.g.Â plots) as catalogs",
    "text": "Independent files (e.g.Â plots) as catalogs\nWe read an individual LAS file as a catalog and perform operations on it.\n\n# Read single file as catalog\nctg_non_norm &lt;- readLAScatalog(folder = \"data/MixedEucaNat.laz\")\n\n# Set options for output files\nopt_output_files(ctg_non_norm) &lt;- paste0(tempdir(),\"/{XCENTER}_{XCENTER}\")\n\n# Write file as .laz\nopt_laz_compression(ctg_non_norm) &lt;- TRUE\n\n# Get random plot locations and clip\nx &lt;- runif(n = 4, min = ctg_non_norm$Min.X, max = ctg_non_norm$Max.X)\ny &lt;- runif(n = 4, min = ctg_non_norm$Min.Y, max = ctg_non_norm$Max.Y)\nrois &lt;- clip_circle(las = ctg_non_norm, xcenter = x, ycenter = y, radius = 10)\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 4 (25%): state âœ“\n#&gt; Chunk 2 of 4 (50%): state âœ“\n#&gt; Chunk 3 of 4 (75%): state âœ“\n#&gt; Chunk 4 of 4 (100%): state âœ“\n\n\n# Read catalog of plots\nctg_plots &lt;- readLAScatalog(tempdir())\n\n# Set independent files option\nopt_independent_files(ctg_plots) &lt;- TRUE\nopt_output_files(ctg_plots) &lt;- paste0(tempdir(),\"/{XCENTER}_{XCENTER}\")\n\n# Generate plot-level terrain models\nrasterize_terrain(las = ctg_plots, res = 1, algorithm = tin())\n#&gt; Warning: There are 951 points flagged 'withheld'.\n#&gt; Chunk 1 of 4 (25%): state âš \n#&gt; Chunk 2 of 4 (50%): state âœ“\n#&gt; Chunk 3 of 4 (75%): state âœ“\n#&gt; Chunk 4 of 4 (100%): state âœ“\n#&gt; Warning: An error occured during the automatic merge of 'catalog_apply'. Merging\n#&gt; is impossible. A list has been returned.\n\n\n\n\n\n\n\n#&gt; [[1]]\n#&gt; [1] \"C:\\\\Users\\\\tgood.stu\\\\AppData\\\\Local\\\\Temp\\\\RtmpEt8kmV/203845.6_203845.6.tif\"\n#&gt; attr(,\"rasterpkg\")\n#&gt; [1] \"terra\"\n#&gt; attr(,\"layernames\")\n#&gt; [1] \"Z\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"C:\\\\Users\\\\tgood.stu\\\\AppData\\\\Local\\\\Temp\\\\RtmpEt8kmV/203876.9_203876.9.tif\"\n#&gt; attr(,\"rasterpkg\")\n#&gt; [1] \"terra\"\n#&gt; attr(,\"layernames\")\n#&gt; [1] \"Z\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"C:\\\\Users\\\\tgood.stu\\\\AppData\\\\Local\\\\Temp\\\\RtmpEt8kmV/203932.2_203932.2.tif\"\n#&gt; attr(,\"rasterpkg\")\n#&gt; [1] \"terra\"\n#&gt; attr(,\"layernames\")\n#&gt; [1] \"Z\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"C:\\\\Users\\\\tgood.stu\\\\AppData\\\\Local\\\\Temp\\\\RtmpEt8kmV/203933.5_203933.5.tif\"\n#&gt; attr(,\"rasterpkg\")\n#&gt; [1] \"terra\"\n#&gt; attr(,\"layernames\")\n#&gt; [1] \"Z\"\n\n\n# Check files\npath &lt;- paste0(tempdir())\nfile_list &lt;- list.files(path, full.names = TRUE)\nfile &lt;- file_list[grep(\"\\\\.tif$\", file_list)][[1]]\n\n# plot dtm\nplot(terra::rast(file))"
  },
  {
    "objectID": "07_engine.html#itd-using-lascatalog",
    "href": "07_engine.html#itd-using-lascatalog",
    "title": "LAScatalog",
    "section": "ITD using LAScatalog",
    "text": "ITD using LAScatalog\nIn this section, we explore Individual Tree Detection (ITD) using the LAScatalog. We first configure catalog options for ITD.\n\n# Set catalog options\nopt_filter(ctg) &lt;- \"-drop_withheld -drop_z_below 0 -drop_z_above 40\"\n\n\nDetect treetops and visualize\nWe detect treetops and visualize the results.\n\n# Detect tree tops and plot\nttops &lt;- locate_trees(las = ctg, algorithm = lmf(ws = 3, hmin = 5))\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\nplot(chm, col = height.colors(50))\nplot(ttops, add = TRUE, cex = 0.1, col = \"black\")\n#&gt; Warning in plot.sf(ttops, add = TRUE, cex = 0.1, col = \"black\"): ignoring all\n#&gt; but the first attribute\n\n\n\n\n\n\n\n\n\n\nSpecify catalog options\nWe specify additional catalog options for ITD.\n\n# Specify more options\nopt_select(ctg) &lt;- \"xyz\"\nopt_chunk_size(ctg) &lt;- 300\nopt_chunk_buffer(ctg) &lt;- 10\n\n# Detect treetops and plot\nttops &lt;- locate_trees(las = ctg, algorithm = lmf(ws = 3, hmin = 5))\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 9 (11.1%): state âœ“\n#&gt; Chunk 2 of 9 (22.2%): state âœ“\n#&gt; Chunk 3 of 9 (33.3%): state âœ“\n#&gt; Chunk 4 of 9 (44.4%): state âœ“\n#&gt; Chunk 5 of 9 (55.6%): state âœ“\n#&gt; Chunk 6 of 9 (66.7%): state âœ“\n#&gt; Chunk 7 of 9 (77.8%): state âœ“\n#&gt; Chunk 8 of 9 (88.9%): state âœ“\n#&gt; Chunk 9 of 9 (100%): state âœ“\nplot(chm, col = height.colors(50))\nplot(ttops, add = TRUE, cex = 0.1, col = \"black\")\n#&gt; Warning in plot.sf(ttops, add = TRUE, cex = 0.1, col = \"black\"): ignoring all\n#&gt; but the first attribute\n\n\n\n\n\n\n\n\n\n\nParallel computing\nIn this section, we explore parallel computing using the lidR package."
  },
  {
    "objectID": "07_engine.html#load-future-library",
    "href": "07_engine.html#load-future-library",
    "title": "LAScatalog",
    "section": "Load future library",
    "text": "Load future library\nWe load the future library to enable parallel processing.\n\nlibrary(future)\n#&gt; Warning: package 'future' was built under R version 4.2.3"
  },
  {
    "objectID": "07_engine.html#specify-catalog-options-1",
    "href": "07_engine.html#specify-catalog-options-1",
    "title": "LAScatalog",
    "section": "Specify catalog options",
    "text": "Specify catalog options\nWe specify catalog options for parallel processing.\n\n# Specify options\nopt_select(ctg) &lt;- \"xyz\"\nopt_chunk_size(ctg) &lt;- 300\nopt_chunk_buffer(ctg) &lt;- 10\n\n# Visualize and summarize the catalog chunks\nplot(ctg, chunk = TRUE)\n\n\n\n\n\n\n\nsummary(ctg)\n#&gt; class       : LAScatalog (v1.2 format 0)\n#&gt; extent      : 207340, 208040, 7357280, 7357980 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S \n#&gt; area        : 489930 mÂ²\n#&gt; points      : 14.49 million points\n#&gt; density     : 29.6 points/mÂ²\n#&gt; density     : 23.2 pulses/mÂ²\n#&gt; num. files  : 25 \n#&gt; proc. opt.  : buffer: 10 | chunk: 300\n#&gt; input opt.  : select: xyz | filter: -drop_withheld -drop_z_below 0 -drop_z_above 40\n#&gt; output opt. : in memory | w2w guaranteed | merging enabled\n#&gt; drivers     :\n#&gt;  - Raster : format = GTiff  NAflag = -999999  \n#&gt;  - stars : NA_value = -999999  \n#&gt;  - SpatRaster : overwrite = FALSE  NAflag = -999999  \n#&gt;  - SpatVector : overwrite = FALSE  \n#&gt;  - LAS : no parameter\n#&gt;  - Spatial : overwrite = FALSE  \n#&gt;  - sf : quiet = TRUE  \n#&gt;  - data.frame : no parameter"
  },
  {
    "objectID": "07_engine.html#single-core-processing",
    "href": "07_engine.html#single-core-processing",
    "title": "LAScatalog",
    "section": "Single core processing",
    "text": "Single core processing\nWe perform tree detection using a single core.\n\n# Process on single core\nfuture::plan(sequential)\n\n# Detect trees\nttops &lt;- locate_trees(las = ctg, algorithm = lmf(ws = 3, hmin = 5))\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 9 (11.1%): state âœ“\n#&gt; Chunk 2 of 9 (22.2%): state âœ“\n#&gt; Chunk 3 of 9 (33.3%): state âœ“\n#&gt; Chunk 4 of 9 (44.4%): state âœ“\n#&gt; Chunk 5 of 9 (55.6%): state âœ“\n#&gt; Chunk 6 of 9 (66.7%): state âœ“\n#&gt; Chunk 7 of 9 (77.8%): state âœ“\n#&gt; Chunk 8 of 9 (88.9%): state âœ“\n#&gt; Chunk 9 of 9 (100%): state âœ“"
  },
  {
    "objectID": "07_engine.html#parallel-processing",
    "href": "07_engine.html#parallel-processing",
    "title": "LAScatalog",
    "section": "Parallel processing",
    "text": "Parallel processing\nWe perform tree detection using multiple cores in parallel.\n\n# Process multi-core\nfuture::plan(multisession)\n\n# Detect trees\nttops &lt;- locate_trees(las = ctg, algorithm = lmf(ws = 3, hmin = 5))\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 9 (11.1%): state âœ“\n#&gt; Chunk 2 of 9 (22.2%): state âœ“\n#&gt; Chunk 3 of 9 (33.3%): state âœ“\n#&gt; Chunk 4 of 9 (44.4%): state âœ“\n#&gt; Chunk 6 of 9 (55.6%): state âœ“\n#&gt; Chunk 5 of 9 (66.7%): state âœ“\n#&gt; Chunk 9 of 9 (77.8%): state âœ“\n#&gt; Chunk 7 of 9 (88.9%): state âœ“\n#&gt; Chunk 8 of 9 (100%): state âœ“"
  },
  {
    "objectID": "07_engine.html#parallel-processing-over-a-network",
    "href": "07_engine.html#parallel-processing-over-a-network",
    "title": "LAScatalog",
    "section": "Parallel processing over a network",
    "text": "Parallel processing over a network\nWe demonstrate how to parallelize processing over a network using future::plan(remote). This is just an example of how one could do this.\n\n# Example of network processing\nfuture::plan(remote, workers = c(\"localhost\", \"bob@132.203.41.87\", \"alice@132.203.41.87\"))\nttops &lt;- locate_trees(ctg, lmf(3, hmin = 5))"
  },
  {
    "objectID": "07_engine.html#revert-to-single-core",
    "href": "07_engine.html#revert-to-single-core",
    "title": "LAScatalog",
    "section": "Revert to single core",
    "text": "Revert to single core\nWe revert to single core processing using future::plan(sequential).\n\n# Back to single core\nfuture::plan(sequential)\n\nThis concludes the tutorial on basic usage, catalog validation, indexing, CHM generation, ABA estimation, data clipping, ITD using catalog, and parallel computing using the lidR package in R."
  },
  {
    "objectID": "07_engine.html#exercises-and-questions",
    "href": "07_engine.html#exercises-and-questions",
    "title": "LAScatalog",
    "section": "Exercises and Questions",
    "text": "Exercises and Questions\n\n\n\n\n\n\nTip\n\n\n\nThis exercise is complex because it involves options not yet described. Be sure to use the lidRbook and package documentation.\n\n\nUsing:\nctg &lt;- readLAScatalog(folder = \"data/Farm_A/\")\n\nE1.\nGenerate a raster of point density for the provided catalog. Hint: Look through the documentation for a function that will do this!\n\n\nE2.\nModify the catalog to have a point density of 10 pts/m2 using the decimate_points() function. If you get an error make sure to read the documentation for decimate_points() and try: using opt_output_file() to write files to a temporary directory.\n\n\nE3.\nGenerate a raster of point density for this new decimated dataset.\n\n\nE4.\nRead the whole decimated catalog as a single las file. The catalog isnâ€™t very big - not recommended for larger datasets!\n\n\nE5.\nRead documentation for the catalog_retile() function and merge the decimated catalog into larger tiles."
  },
  {
    "objectID": "08_engine2.html",
    "href": "08_engine2.html",
    "title": "LAScatalog processing engine",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "08_engine2.html#relevant-resources",
    "href": "08_engine2.html#relevant-resources",
    "title": "LAScatalog processing engine",
    "section": "Relevant resources:",
    "text": "Relevant resources:\nCode\n\nlidRbook section: Engine\nlidRbook section: Thinking outside the box"
  },
  {
    "objectID": "08_engine2.html#overview",
    "href": "08_engine2.html#overview",
    "title": "LAScatalog processing engine",
    "section": "Overview",
    "text": "Overview\nThis code showcases the LASCATALOG PROCESSING ENGINE, which efficiently and in parallel applies various functions to LiDAR catalogs. It introduces the catalog_apply() function for processing LiDAR data in a catalog. The code includes routines to detect trees and calculate metrics on the LiDAR catalog."
  },
  {
    "objectID": "08_engine2.html#environment",
    "href": "08_engine2.html#environment",
    "title": "LAScatalog processing engine",
    "section": "Environment",
    "text": "Environment\n\n# Clear environment\nrm(list = ls(globalenv()))\n\n# Load packages\nlibrary(lidR)\n#&gt; Warning: package 'lidR' was built under R version 4.2.3\nlibrary(terra)\n#&gt; Warning: package 'terra' was built under R version 4.2.3\nlibrary(future)\n#&gt; Warning: package 'future' was built under R version 4.2.3"
  },
  {
    "objectID": "08_engine2.html#basic-usage",
    "href": "08_engine2.html#basic-usage",
    "title": "LAScatalog processing engine",
    "section": "Basic Usage",
    "text": "Basic Usage\nIn this section, we will cover the basic usage of the lidR package, including reading LiDAR data, visualization, and inspecting metadata.\n\nBasic Usage of lidR Package\nThis section introduces the basic usage of the lidR package for reading and visualizing LiDAR data, as well as inspecting metadata.\n\n\nReading and Visualizing LiDAR Data\nWe start by reading a LAS catalog and inspecting one of its LAS files.\n\n# Read a LAS catalog\nctg &lt;- readLAScatalog(folder = \"data/Farm_A/\")\n\n# Inspect the first LAS file in the catalog\nlas_file &lt;- ctg$filename[1]\nlas &lt;- readLAS(las_file)\n#&gt; Warning: There are 167254 points flagged 'withheld'.\nlas\n#&gt; class        : LAS (v1.2 format 0)\n#&gt; memory       : 28.7 Mb \n#&gt; extent       : 207340, 207480, 7357280, 7357420 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : SIRGAS 2000 / UTM zone 23S \n#&gt; area         : 19600 mÂ²\n#&gt; points       : 578.3 thousand points\n#&gt; density      : 29.5 points/mÂ²\n#&gt; density      : 22.81 pulses/mÂ²\n\n\n\nVisualizing LiDAR Data\nWe visualize the LiDAR data from the selected LAS file using a 3D plot.\n\n# Visualize the LiDAR data in 3D\nplot(las, bg = \"white\")\n\n\n\ncatalog_apply() Function\nThis section demonstrates the use of the catalog_apply() function for efficient processing of LiDAR data within a LAS catalog.\n\n\nProblem Statement\nWe start by addressing a common problem - how can we apply operations to LAS data in a catalog?\n\n# Read a LAS file from the catalog and filter surface points\nlas_file &lt;- ctg$filename[16]\nlas &lt;- readLAS(files = las_file, filter = \"-drop_withheld -drop_z_below 0 -drop_z_above 40\")\nsurflas &lt;- filter_surfacepoints(las = las, res = 1)\n\n\n\nVisualizing LiDAR Data\nWe visualize the selected LiDAR data, including both the original data and the surface points.\n# Visualize the LiDAR data with a default color palette\nplot(las)\n\n\n\n\n\n\n\n\n\n# Visualize the surface points using a default color palette\nplot(surflas)\n\n\n\n\n\n\n\n\n\n\n\nCalculating Rumple Index\nWe calculate the rumple index using the pixel_metrics() function.\n\n# Generate Area-based metrics\nri &lt;- pixel_metrics(las = las, ~rumple_index(X,Y,Z), res = 10)\nplot(ri)\n\n\n\n\n\n\n\n\n\n\nSolution: LAScatalog Processing Engine\nThis section introduces the LAScatalog processing engine, a powerful tool for efficient processing of LAS data within a catalog.\n\n\nBasic Usage of the catalog_apply() Function\nWe demonstrate the basic usage of the catalog_apply() function with a simple user-defined function.\n\n# User-defined function for processing chunks\nroutine &lt;- function(chunk){\n  las &lt;- readLAS(chunk)\n  if (is.empty(las)) return(NULL)\n  \n  # Perform computation\n  m &lt;- pixel_metrics(las = las, func = ~max(Z), res = 20)\n  output &lt;- terra::crop(x = m, terra::ext(chunk))\n  \n  return(output)\n}\n\n# Initialize parallel processing\nplan(multisession)\n\n# Specify catalog options\nopt_filter(ctg) &lt;- \"-drop_withheld\"\n\n# Apply routine to catalog\nout &lt;- catalog_apply(ctg = ctg, FUN = routine)\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 20 of 25 (76%): state âœ“\n#&gt; Chunk 19 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\n\n# Inspect the output list\nout[1:5]\n#&gt; [[1]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 7, 7, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 207480, 7357280, 7357420  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   :  8.13 \n#&gt; max value   : 62.81 \n#&gt; \n#&gt; [[2]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 7, 7, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 207480, 7357420, 7357560  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :   V1 \n#&gt; min value   :  6.4 \n#&gt; max value   : 58.8 \n#&gt; \n#&gt; [[3]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 7, 7, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 207480, 7357560, 7357700  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   :  9.24 \n#&gt; max value   : 30.68 \n#&gt; \n#&gt; [[4]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 7, 7, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 207480, 7357700, 7357840  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   : 23.23 \n#&gt; max value   : 89.93 \n#&gt; \n#&gt; [[5]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 7, 7, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 207480, 7357840, 7357980  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   : 24.65 \n#&gt; max value   : 28.51\n\n# Use the engine-supported method for merging\noptions &lt;- list(automerge = TRUE)\nout &lt;- catalog_apply(ctg = ctg, FUN = routine, .options = options)\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\nprint(out)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 35, 35, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 207340, 208040, 7357280, 7357980  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; name        :    V1 \n#&gt; min value   :  0.40 \n#&gt; max value   : 93.35\n\n\n\nUser-Defined Functions for Processing\nWe demonstrate the use of user-defined functions to process LiDAR data within a catalog.\n\n# User-defined function for rumple index calculation\nroutine_rumple &lt;- function(chunk, res1 = 10, res2 = 1){\n  las &lt;-  readLAS(chunk)\n  if (is.empty(las)) return(NULL)\n  bbox &lt;- terra::ext(chunk)\n  \n  las &lt;- filter_surfacepoints(las = las, res = res2)\n  ri  &lt;- pixel_metrics(las = las, ~rumple_index(X,Y,Z), res1)\n  \n  output &lt;- terra::crop(x = ri, y = bbox)\n  return(output)\n}\n\n# Set catalog options\nopt_select(ctg) &lt;- \"xyz\"\nopt_filter(ctg) &lt;- \"-drop_withheld -drop_z_below 0 -drop_z_above 40\"\nopt_chunk_buffer(ctg) &lt;- 0\nopt_chunk_size(ctg) &lt;- 0\n\n# Specify options for merging\noptions &lt;- list(automerge = TRUE, alignment = 10)\n\n# Apply the user-defined function to the catalog\nri &lt;- catalog_apply(ctg = ctg, FUN = routine_rumple, res1 = 10, res2 = 0.5, .options = options)\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\n\n# Plot the output\nplot(ri, col = height.colors(50))"
  },
  {
    "objectID": "08_engine2.html#catalog_apply---example-2",
    "href": "08_engine2.html#catalog_apply---example-2",
    "title": "LAScatalog processing engine",
    "section": "catalog_apply() - Example 2",
    "text": "catalog_apply() - Example 2\nIn this section, we provide another example of using the catalog_apply() function to detect trees and calculate metrics on a catalog.\n\nDefining Routines for Tree Detection and Metrics\nWe define a routine that detects trees, calculates metrics, and returns relevant data.\n\n# User-defined routine for tree detection and metrics\nroutine_trees &lt;- function(chunk) {\n  # Read in the chunk and check for emptiness\n  las &lt;- readLAS(chunk)\n  if (is.empty(las)) return(NULL)\n  \n  # Get the chunk bounding box\n  bbox &lt;- sf::st_bbox(obj = chunk)\n\n  # Filter surface points and create canopy height model (CHM)\n  las &lt;- filter_surfacepoints(las, res = 0.5)\n  chm &lt;- rasterize_canopy(las = las, res = 0.5, algorithm = p2r())\n\n  # Detect and segment trees\n  ttops &lt;- locate_trees(las = las, algorithm = lmf(ws = 3, hmin = 5))\n  las_trees &lt;- segment_trees(las = las, algorithm = dalponte2016(chm = chm, treetops = ttops))\n  \n  # Generate metrics for each tree\n  p &lt;- crown_metrics(las = las_trees, func = .stdtreemetrics)\n  p &lt;- sf::st_crop(x = p, y = bbox)\n\n  # Delineate convex hulls\n  m &lt;- delineate_crowns(las_trees)\n  output &lt;- m[m$treeID %in% p$treeID,]\n\n  return(output)\n}\n\n# Set options for the catalog\nopt_chunk_buffer(ctg) &lt;- 15\noptions &lt;- list(automerge = TRUE) # Merge all outputs\n\n# Apply the function to the catalog\nm &lt;- catalog_apply(ctg = ctg, FUN = routine_trees, .options = options)\n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 1 of 25 (4%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 2 of 25 (8%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 3 of 25 (12%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 4 of 25 (16%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 5 of 25 (20%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 6 of 25 (24%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 7 of 25 (28%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 8 of 25 (32%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 9 of 25 (36%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 11 of 25 (40%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 10 of 25 (44%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 12 of 25 (48%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 13 of 25 (52%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 14 of 25 (56%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 15 of 25 (60%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 16 of 25 (64%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 17 of 25 (68%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 20 of 25 (72%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 18 of 25 (76%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 19 of 25 (80%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 21 of 25 (84%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 22 of 25 (88%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 23 of 25 (92%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n#&gt; Chunk 24 of 25 (96%): state âš \n#&gt; Warning: attribute variables are assumed to be spatially constant throughout all\n#&gt; geometries\n\n\n\n\n\n\n\n#&gt; Chunk 25 of 25 (100%): state âš \n\n# View and visualize the output\nm\n#&gt; class       : SpatialPolygonsDataFrame \n#&gt; features    : 30270 \n#&gt; extent      : 207340, 208040, 7357280, 7357980  (xmin, xmax, ymin, ymax)\n#&gt; crs         : +proj=utm +zone=23 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n#&gt; variables   : 4\n#&gt; names       : treeID,      XTOP,       YTOP,  ZTOP \n#&gt; min values  :      1, 207340.04, 7357280.04,     5 \n#&gt; max values  :   2456, 208039.91, 7357979.98, 39.87\nplot(m)\n\n\n\n\n\n\n\n\n# End parallel processing\nfuture::plan(sequential)\n\n\n\n\n\n\n\nThinking outside the box\n\n\n\nThe LAScatalog engine is versatile! The functions that can be applied to LiDAR data are infinite - leverage the flexibility of lidR and create software that pushes the boundaries of research in forest inventory and management!"
  },
  {
    "objectID": "08_engine2.html#exercises",
    "href": "08_engine2.html#exercises",
    "title": "LAScatalog processing engine",
    "section": "Exercises",
    "text": "Exercises\n\nE1.\nUnderstanding Chunk Filtering\nOn line 111 (routine_trees function) m &lt;- m[m$treeID %in% p$treeID,] is used. Explain the purpose of this line. To understand its impact, modify the function to exclude this line and observe the results. You can use the catalog_select() function to choose a subset of tiles for testing.\nsubctg &lt;- catalog_select(ctg)\n\n\nE2.\nImplement Noise Filtering\n\nExplain the purpose of the filter_noise() function.\nCreate a user-defined function to apply noise filtering using the catalog_apply() function.\nMake sure to consider buffered points when using lidRâ€™s filter_* functions.\n\n\n\nE3.\nFlightline Convex Hull Application\nDesign an application to retrieve the convex hull of each flightline using the concaveman::concaveman() function and functions from the sf package.\n\nBegin by designing a test function that works on a single LAS object.\nApply the function to a collection of LAS files.\nVisualize the results using the flightlinesâ€™ shapefile.\n\nflightlines &lt;- st_read(\"data/flightlines.shp\")\nplot(flightlines, col = sf.colors(6, alpha = 0.5))\nplot(flightlines[3,])"
  },
  {
    "objectID": "08_engine2.html#conclusion",
    "href": "08_engine2.html#conclusion",
    "title": "LAScatalog processing engine",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes the tutorial on using the catalog_apply function in the lidR package to efficiently process LAS data within a catalog."
  },
  {
    "objectID": "09_solutions.html",
    "href": "09_solutions.html",
    "title": "Excercise Solutions",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "09_solutions.html#resources",
    "href": "09_solutions.html#resources",
    "title": "Excercise Solutions",
    "section": "Resources",
    "text": "Resources\nCode"
  },
  {
    "objectID": "09_solutions.html#las",
    "href": "09_solutions.html#las",
    "title": "Excercise Solutions",
    "section": "1-LAS",
    "text": "1-LAS\n\n# Load packages\nlibrary(lidR)\nlibrary(sf)\nlibrary(terra)\n\n\nE1.\nWhat are withheld points? Where are they in our pointcloud?\n\n\nCode\n# According to ASPRS LAS specification http://www.asprs.org/wp-content/uploads/2019/07/LAS_1_4_r15.pdf page 18 \"a point # that should not be included in processing (synonymous with Deleted)\"\n\n# They are on the edges. It looks like they correspond to a buffer. LAStools makes use of the withheld bit to flag some # points. Without more information on former processing step it is hard to say.\n\n\n\n\nE2.\nRead the file dropping the withheld points.\n\n\nCode\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", filter = \"-drop_withheld\")\nplot(las)\n\n\n\n\nE3.\nThe withheld points seem to be legitimate points that we want to keep.\nTry to load the file including the withheld points but get rid of the warning (without using suppressWarnings()). Hint: Check available -set_withheld filters in readLAS(filter = \"-h\")\n\n\nCode\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", filter = \"-set_withheld_flag 0\")\nplot(las, color = \"Withheld_flag\")\n\n\n\n\nE4.\nLoad only the ground points and plot the point-cloud coloured by the returnnumber of the point. Do it loading the strict minimal amount of memory (4.7 Mb). Hint: use ?lidR::readLAS and see what select options might help.\n\n\nCode\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", filter = \"-keep_class 2 -set_withheld_flag 0\", select = \"r\")\nplot(las, color = \"ReturnNumber\", legend = T)\nformat(object.size(las), \"Mb\")\n#&gt; [1] \"4.6 Mb\""
  },
  {
    "objectID": "09_solutions.html#roi",
    "href": "09_solutions.html#roi",
    "title": "Excercise Solutions",
    "section": "2-ROI",
    "text": "2-ROI\n\nplots &lt;- st_read(\"data/shapefiles/MixedEucaNatPlot.shp\")\n#&gt; Reading layer `MixedEucaNatPlot' from data source \n#&gt;   `C:\\Users\\tgood.stu\\Documents\\GitHub\\lidRtutorial\\data\\shapefiles\\MixedEucaNatPlot.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 5 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 203879.6 ymin: 7358932 xmax: 203960.6 ymax: 7359033\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\nplot(las@header, map = FALSE)\nplot(plots, add = TRUE)\n\n\n\n\n\n\n\n\n\nE1.\nClip the 5 plots with a radius of 11.3 m,\n\n\nCode\ninventory &lt;- clip_roi(las, plots, radius = 11.3)\nplot(inventory[[2]])\n\n\n\n\nE2.\nClip a transect from A c(203850, 7358950) to B c(203950, 7959000).\n\n\nCode\ntr &lt;- clip_transect(las, c(203850, 7358950), c(203950, 7359000), width = 5)\nplot(tr, axis = T)\n\n\n\n\nE3.\nClip a transect from A c(203850, 7358950) to B c(203950, 7959000) but reorient it so it is no longer on the XY diagonal. Hint = ?clip_transect\n\n\nCode\nptr &lt;- clip_transect(las, c(203850, 7358950), c(203950, 7359000), width = 5, xz = TRUE)\nplot(tr, axis = T)\nplot(ptr, axis = T)\nplot(ptr$X, ptr$Z, cex = 0.25, pch = 19, asp = 1)"
  },
  {
    "objectID": "09_solutions.html#aba",
    "href": "09_solutions.html#aba",
    "title": "Excercise Solutions",
    "section": "3-ABA",
    "text": "3-ABA\n\nlas &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", select = \"*\",  filter = \"-set_withheld_flag 0\")\n\n\nE1.\nAssuming that biomass is estimated using the equation B = 0.5 * mean Z + 0.9 * 90th percentile of Z applied on first returns only, map the biomass.\n\n\nCode\nB &lt;- pixel_metrics(las, ~0.5*mean(Z) + 0.9*quantile(Z, probs = 0.9), 10, filter = ~ReturnNumber == 1L)\nplot(B, col = height.colors(50))\n\n\n\n\n\n\n\n\n\nCode\n\nB &lt;- pixel_metrics(las, .stdmetrics_z, 10)\nB &lt;- 0.5*B[[\"zmean\"]] + 0.9*B[[\"zq90\"]]\nplot(B, col = height.colors(50))\n\n\n\n\n\n\n\n\n\nCode\n\npixel_metrics(las, ~as.list(quantile(Z), 10))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 8, 8, 5  (nrow, ncol, nlyr)\n#&gt; resolution  : 20, 20  (x, y)\n#&gt; extent      : 203820, 203980, 7358900, 7359060  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : SIRGAS 2000 / UTM zone 23S (EPSG:31983) \n#&gt; source(s)   : memory\n#&gt; names       : 0%,   25%,   50%,  75%,  100% \n#&gt; min values  :  0,  0.00,  0.00,  0.0,  0.79 \n#&gt; max values  :  0, 12.32, 17.19, 27.4, 34.46\n\n\n\n\nE2.\nMap the density of ground returns at a 5 m resolution with pixel_metrics(filter = ~Classification == LASGROUND).\n\n\nCode\nGND &lt;- pixel_metrics(las, ~length(Z)/25, res = 5, filter = ~Classification == LASGROUND)\nplot(GND, col = heat.colors(50))\n\n\n\n\n\n\n\n\n\n\n\nE3.\nMap pixels that are flat (planar) using stdshapemetrics. These could indicate potential roads.\n\n\nCode\nm &lt;- pixel_metrics(las, .stdshapemetrics, res = 3)\nplot(m[[\"planarity\"]], col = heat.colors(50))\n\n\n\n\n\n\n\n\n\nCode\nflat &lt;- m[[\"planarity\"]] &gt; 0.85\nplot(flat)"
  },
  {
    "objectID": "09_solutions.html#dtm",
    "href": "09_solutions.html#dtm",
    "title": "Excercise Solutions",
    "section": "5-DTM",
    "text": "5-DTM\n\nE1.\nPlot and compare these two normalized point-clouds. Why do they look different? Fix that. Hint: filter.\nSome non ground points are below 0. It can be slightly low noise point not classified as ground by the data provider. This low points not being numerous and dark blue we hardly see them\n\n\nCode\nlas1 &lt;- readLAS(\"data/MixedEucaNat.laz\", filter = \"-set_withheld_flag 0\")\nnlas1 &lt;- normalize_height(las1, tin())\nnlas2 &lt;- readLAS(\"data/MixedEucaNat_normalized.laz\", filter = \"-set_withheld_flag 0\")\nplot(nlas1)\nplot(nlas2)\n\nnlas1 &lt;- filter_poi(nlas1, Z &gt; -0.1)\nplot(nlas1)\n\n\n\n\nE2.\nClip a plot somewhere in MixedEucaNat.laz (the non-normalized file).\n\n\nCode\ncirc &lt;- clip_circle(las, 203930, 7359000, 25)\nplot(circ)\n\n\n\n\nE3.\nCompute a DTM for this plot. Which method are you choosing and why?\n\n\nCode\ndtm &lt;- grid_terrain(circ, 0.5, kriging())\n#&gt; Warning: There were 2 degenerated ground points. Some X Y coordinates were\n#&gt; repeated but with different Z coordinates. min Z were retained.\nplot_dtm3d(dtm)\n\n\n\n\nE4.\nCompute a DSM (digital surface model). Hint: Look back to how you made a CHM.\n\n\nCode\ndsm &lt;- grid_canopy(circ, 1, p2r(0.1))\nplot(dsm, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n\n\nE5.\nNormalize the plot.\n\n\nCode\nncirc &lt;- circ - dtm\nplot(ncirc)\n\n\n\n\nE6.\nCompute a CHM.\n\n\nCode\nchm &lt;- grid_canopy(ncirc, 1, p2r(0.1))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n\n\nE7.\nEstimate some metrics of interest in this plot with cloud_metric()\n\n\nCode\nmetrics &lt;- cloud_metrics(ncirc, .stdmetrics_z)\nmetrics\n#&gt; $zmax\n#&gt; [1] 31.41\n#&gt; \n#&gt; $zmean\n#&gt; [1] 11.11374\n#&gt; \n#&gt; $zsd\n#&gt; [1] 11.44308\n#&gt; \n#&gt; $zskew\n#&gt; [1] 0.4123246\n#&gt; \n#&gt; $zkurt\n#&gt; [1] 1.42725\n#&gt; \n#&gt; $zentropy\n#&gt; [1] NA\n#&gt; \n#&gt; $pzabovezmean\n#&gt; [1] 42.39526\n#&gt; \n#&gt; $pzabove2\n#&gt; [1] 60.61408\n#&gt; \n#&gt; $zq5\n#&gt; [1] 0\n#&gt; \n#&gt; $zq10\n#&gt; [1] 0\n#&gt; \n#&gt; $zq15\n#&gt; [1] 0\n#&gt; \n#&gt; $zq20\n#&gt; [1] 0\n#&gt; \n#&gt; $zq25\n#&gt; [1] 0\n#&gt; \n#&gt; $zq30\n#&gt; [1] 0\n#&gt; \n#&gt; $zq35\n#&gt; [1] 1.18\n#&gt; \n#&gt; $zq40\n#&gt; [1] 2.14\n#&gt; \n#&gt; $zq45\n#&gt; [1] 3.62\n#&gt; \n#&gt; $zq50\n#&gt; [1] 5.25\n#&gt; \n#&gt; $zq55\n#&gt; [1] 8.903\n#&gt; \n#&gt; $zq60\n#&gt; [1] 13.12\n#&gt; \n#&gt; $zq65\n#&gt; [1] 18.32\n#&gt; \n#&gt; $zq70\n#&gt; [1] 22.1\n#&gt; \n#&gt; $zq75\n#&gt; [1] 24.31\n#&gt; \n#&gt; $zq80\n#&gt; [1] 25.62\n#&gt; \n#&gt; $zq85\n#&gt; [1] 26.7\n#&gt; \n#&gt; $zq90\n#&gt; [1] 27.53\n#&gt; \n#&gt; $zq95\n#&gt; [1] 28.39\n#&gt; \n#&gt; $zpcum1\n#&gt; [1] 18.12365\n#&gt; \n#&gt; $zpcum2\n#&gt; [1] 30.03214\n#&gt; \n#&gt; $zpcum3\n#&gt; [1] 35.78736\n#&gt; \n#&gt; $zpcum4\n#&gt; [1] 41.12687\n#&gt; \n#&gt; $zpcum5\n#&gt; [1] 45.38727\n#&gt; \n#&gt; $zpcum6\n#&gt; [1] 49.90123\n#&gt; \n#&gt; $zpcum7\n#&gt; [1] 56.24318\n#&gt; \n#&gt; $zpcum8\n#&gt; [1] 68.07501\n#&gt; \n#&gt; $zpcum9\n#&gt; [1] 91.75045"
  },
  {
    "objectID": "09_solutions.html#its",
    "href": "09_solutions.html#its",
    "title": "Excercise Solutions",
    "section": "6-ITS",
    "text": "6-ITS\nUsing:\n\nlas &lt;- readLAS(\"data/example_corrupted.laz\", select = \"xyz\")\ncol1 &lt;- height.colors(50)\n\n\nE1.\nRun las_check() and fix the errors.\n\n\nCode\nlas_check(las)\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\n#&gt;  \u001b[1;33m   âš  202348 points are duplicated and share XYZ coordinates with other points\u001b[0m\n#&gt;   - Checking degenerated ground points...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking attribute population...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking negative outliers...\n#&gt;  \u001b[1;33m   âš  77 points below 0\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\u001b[0;31m no\u001b[0m\n\nlas &lt;- filter_duplicates(las = las)\n\nlas_check(las)\n#&gt; \n#&gt;  Checking the data\n#&gt;   - Checking coordinates...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates range...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinates quantization...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking attributes type...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking NumberOfReturns validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking RGB validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking absence of NAs...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking duplicated points...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking degenerated ground points...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking attribute population...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking gpstime incoherances\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking flag attributes...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking user data attribute...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking the header\n#&gt;   - Checking header completeness...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking scale factor validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking point data format ID validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking extra bytes attributes validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking the bounding box validity...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking coordinate reference system...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking header vs data adequacy\n#&gt;   - Checking attributes vs. point format...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header bbox vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header number of points vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;   - Checking header return number vs. actual content...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking coordinate reference system...\n#&gt;   - Checking if the CRS was understood by R...\u001b[0;32m âœ“\u001b[0m\n#&gt;  Checking preprocessing already done \n#&gt;   - Checking ground classification...\u001b[0;37m skipped\u001b[0m\n#&gt;   - Checking normalization...\u001b[0;32m yes\u001b[0m\n#&gt;   - Checking negative outliers...\n#&gt;  \u001b[1;33m   âš  41 points below 0\u001b[0m\n#&gt;   - Checking flightline classification...\u001b[0;37m skipped\u001b[0m\n#&gt;  Checking compression\n#&gt;   - Checking attribute compression...\u001b[0;31m no\u001b[0m\n\n\n\n\nE2.\nFind the trees and count the trees.\n\n\nCode\nttops &lt;- locate_trees(las = las, algorithm = lmf(ws = 3, hmin = 5))\nx &lt;- plot(las)\nadd_treetops3d(x = x, ttops = ttops)\n\n\n\n\nE3.\nCompute and map the density of trees with a 10 m resolution.\n\n\nCode\nr &lt;- terra::rast(x = ttops)\nterra::res(r) &lt;- 10\nr &lt;- terra::rasterize(x = ttops, y = r, \"treeID\", fun = 'count')\nplot(r, col = viridis::viridis(20))\n\n\n\n\n\n\n\n\n\n\n\nE4.\nSegment the trees.\n\n\nCode\nchm &lt;- grid_canopy(las = las, res = 0.5, algorithm = p2r(subcircle = 0.15))\nplot(chm, col = col1)\n\n\n\n\n\n\n\n\n\nCode\nttops &lt;- locate_trees(las = chm, algorithm = lmf(ws = 2.5))\nlas &lt;- segment_trees(las = las, dalponte2016(chm = chm, treetops = ttops))\n\nplot(las, color = \"treeID\")\n\n\n\n\nE5.\nAssuming that a value of interest of a tree can be estimated using the crown area and the mean Z of the points with the formula 2.5 * area + 3 * mean Z. Estimate the value of interest of each tree.\n\n\nCode\nvalue_of_interest &lt;- function(x,y,z)\n{\n  m &lt;- stdtreemetrics(x,y,z)\n  avgz &lt;- mean(z)\n  v &lt;- 2.5*m$convhull_area + 3 * avgz\n  return(list(V = v))\n}\n\nV &lt;- crown_metrics(las = las, func = ~value_of_interest(X,Y,Z))\nplot(x = V[\"V\"])\n\n\n\n\n\n\n\n\n\nCode\n\n# 6. Map the total biomass at a resolution of 10 m. The output is a mixed of ABA and ITS\n\nVtot &lt;- rasterize(V, r, \"V\", fun = \"sum\")\nplot(Vtot, col = viridis::viridis(20))"
  },
  {
    "objectID": "09_solutions.html#lasctalog",
    "href": "09_solutions.html#lasctalog",
    "title": "Excercise Solutions",
    "section": "7-LASCTALOG",
    "text": "7-LASCTALOG\nThis exercise is complex because it involves options not yet described. Be sure to use the lidRbook and package documentation.\nhttps://cran.r-project.org/web/packages/lidR/lidR.pdf https://r-lidar.github.io/lidRbook/index.html\nUsing:\n\nctg &lt;- readLAScatalog(folder = \"data/Farm_A/\")\n\n\nE1.\nGenerate a raster of point density for the provided catalog. Hint: Look through the documentation for a function that will do this!\n\n\nCode\nctg &lt;- readLAScatalog(\"data/Farm_A/\", filter = \"-drop_withheld -drop_z_below 0 -drop_z_above 40\")\nD1 &lt;- rasterize_density(las = ctg, res = 4)\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\nplot(D1, col = heat.colors(50))\n\n\n\n\n\n\n\n\n\n\nE2.\nModify the catalog to have a point density of 10 pts/m2 using the decimate_points() function. If you get an error make sure to read the documentation for decimate_points() and try: using opt_output_file() to write files to a temporary directory.\nhttps://r-lidar.github.io/lidRbook/engine.html#engine-dtm-ondisk\n\n\nCode\nnewctg &lt;- decimate_points(las = ctg, algorithm = homogenize(density = 10, res = 5))\n#&gt;  Error: This function requires that the LAScatalog provides an output file template.\n\n\n\n\nCode\nopt_filter(ctg) &lt;- \"-drop_withheld\"\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{ORIGINALFILENAME}\")\nnewctg &lt;- decimate_points(las = ctg, algorithm = homogenize(density = 10, res = 5))\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\n\n\n\nE3.\nGenerate a raster of point density for this new decimated dataset.\n\n\nCode\nopt_output_files(newctg) &lt;- \"\"\nD2 &lt;- grid_density(las = newctg, res = 4)\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\nplot(D2, col = heat.colors(50))\n\n\n\n\n\n\n\n\n\n\nE4.\nRead the whole decimated catalog as a single las file. The catalog isnâ€™t very big - not recommended for larger data sets!\n\n\nCode\nlas &lt;- readLAS(newctg)\nplot(las)\n\n\n\n\nE5.\nRead documentation for the catalog_retile() function and merge the dataset into larger tiles. Use ctg metadata to align new chunks to the lower left corner of the old ones. Hint: Visualize the chunks and use opt_chunk_* options.\n\n\nCode\nopt_chunk_size(ctg) &lt;- 280\nopt_chunk_buffer(ctg) &lt;- 0\nopt_chunk_alignment(ctg) &lt;- c(min(ctg$Min.X), min(ctg$Min.Y))\nplot(ctg, chunk = T)\n\nopt_output_files(ctg) &lt;- \"{tempdir()}/PRJ_A_{XLEFT}_{YBOTTOM}\"\nnewctg &lt;- catalog_retile(ctg = ctg)\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 9 (11.1%): state âœ“\n#&gt; Chunk 2 of 9 (22.2%): state âœ“\n#&gt; Chunk 3 of 9 (33.3%): state âœ“\n#&gt; Chunk 4 of 9 (44.4%): state âœ“\n#&gt; Chunk 5 of 9 (55.6%): state âœ“\n#&gt; Chunk 6 of 9 (66.7%): state âœ“\n#&gt; Chunk 7 of 9 (77.8%): state âœ“\n#&gt; Chunk 8 of 9 (88.9%): state âœ“\n#&gt; Chunk 9 of 9 (100%): state âœ“\nplot(newctg)"
  },
  {
    "objectID": "09_solutions.html#engine",
    "href": "09_solutions.html#engine",
    "title": "Excercise Solutions",
    "section": "8-ENGINE",
    "text": "8-ENGINE\n\nE1.\nIn example 2 (section B) what does last line m &lt;- m[m$treeID %in% p$treeID,] do? Adjust the function to not include that line to see what happens (use catalog_select() to select 4 tiles to test on).\n\n\nCode\n# Subset catalog\nsubctg &lt;- catalog_select(ctg)\n\n# without line\nroutine_trees_test &lt;- function(chunk) {\n  # Read in check, check NULL status, get bbox\n  las &lt;- readLAS(chunk)\n  if (is.empty(las)) return(NULL)\n  bbox &lt;- st_bbox(chunk)\n  \n  # Filter surface points and generate chm\n  las &lt;- filter_surfacepoints(las, res = 0.5)\n  chm &lt;- rasterize_canopy(las = las, res = 0.5, algorithm = p2r())\n  \n  # Tree detection, segmentation, metrics\n  ttops &lt;- locate_trees(las = las, algorithm = lmf(ws = 3, hmin = 5))\n  las_trees &lt;- segment_trees(las = las, algorithm = dalponte2016(chm = chm, treetops = ttops))\n  p &lt;- crown_metrics(las = las_trees, func = .stdtreemetrics)\n  \n  # Remove buffer\n  p &lt;- sf::st_crop(x = p, y = bbox)\n  \n  # Delineate crowns\n  output &lt;- delineate_crowns(las_trees)\n  \n  #output &lt;- m[m$treeID %in% p$treeID,]\n  \n  return(output)\n}\n\noptions &lt;-  list(automerge = TRUE)\nm &lt;-  catalog_apply(subctg, routine_trees_test, .options = options)\nplot(m, col = rgb(0,0,1,0.3))\n\n\n\n\nCode\nctg &lt;-  readLAScatalog(\"data/Farm_A/\")\nopt_select(ctg) &lt;- \"xyz\"\nopt_filter(ctg) &lt;- \"-drop_withheld -drop_z_below 0 -drop_z_above 40\"\nopt_chunk_buffer(ctg) &lt;- 15\nopt_chunk_size(ctg) &lt;- 0\nsubctg &lt;-  catalog_select(ctg)\noptions &lt;-  list(automerge = TRUE)\nm &lt;- catalog_apply(subctg, routine_trees_test, .options = options)\n\nplot(m, col = rgb(0,0,1,0.3))\n\n\n\n\nE2.\nThe following is a simple (and a bit naive) function to remove high noise points. - Explain what this function does - Create a user-defined function to apply using catalog_apply() - Hint: Dont forget about buffered pointsâ€¦ remember lidR::filter_* functions.\n\n\nCode\nfilter_noise &lt;- function(las, sensitivity)\n{\n  p95 &lt;- pixel_metrics(las, ~quantile(Z, probs = 0.95), 10)\n  las &lt;- merge_spatial(las, p95, \"p95\")\n  las &lt;- filter_poi(las, Z &lt; 1+p95*sensitivity, Z &gt; -0.5)\n  las$p95 &lt;- NULL\n  return(las)\n}\n\nfilter_noise_collection = function(cl, sensitivity)\n{\n  las &lt;- readLAS(cl)\n  if (is.empty(las)) return(NULL)\n  las &lt;- filter_noise(las, sensitivity)\n  las &lt;- filter_poi(las, buffer == 0L)\n  return(las)\n}\n\nctg = readLAScatalog(\"data/Farm_A/\")\nopt_select(ctg) &lt;- \"*\"\nopt_filter(ctg) &lt;- \"-drop_withheld -drop_\"\nopt_output_files(ctg) &lt;- \"{tempdir()}/*\"\nopt_chunk_buffer(ctg) &lt;- 20\nopt_chunk_size(ctg) &lt;- 0\n\noptions &lt;- list(automerge = TRUE)\noutput &lt;- catalog_apply(ctg, filter_noise_collection, sensitivity = 1.2, .options = options)\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 23 of 25 (92%): state âœ“\n#&gt; Chunk 24 of 25 (96%): state âœ“\n#&gt; Chunk 25 of 25 (100%): state âœ“\n\nlas &lt;- readLAS(output)\nplot(las)\n\n\n\nE3.\nDesign an application that retrieves the convex hull of each flightline (hard). Use the concaveman::concaveman() function, adn functions from sf. Start by designing a test function that works on a LAS object and later apply on the collection. The output should look like:\n\nflightlines &lt;- st_read(\"data/flightlines.shp\")\n#&gt; Reading layer `flightlines' from data source \n#&gt;   `C:\\Users\\tgood.stu\\Documents\\GitHub\\lidRtutorial\\data\\flightlines.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 6 features and 1 field\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 207340 ymin: 7357280 xmax: 208040 ymax: 7357980\n#&gt; Projected CRS: SIRGAS 2000 / UTM zone 23S\nplot(flightlines, col = sf.colors(6, alpha = 0.5))\n\n\n\n\n\n\n\nplot(flightlines[3,])\n\n\n\n\n\n\n\n\n\n\nCode\n# Read the catalog\nctg &lt;- readLAScatalog(\"data/Farm_A/\")\n\n# Read a single file to perform tests\nlas &lt;- readLAS(ctg$filename[16], select = \"xyzp\", filter = \"-drop_withheld -drop_z_below 0 -drop_z_above 40\")\n\n# Define a function capable of building the hull from the XY of a given PointSourceID\nenveloppes &lt;- function(x,y, psi)\n{\n  hull &lt;- concaveman::concaveman(cbind(x,y), length_threshold = 10)\n  hull &lt;- sf::st_polygon(list(hull))\n  hull &lt;- sf::st_sfc(hull)\n  hull &lt;- sf::st_simplify(hull, dTolerance = 1)\n  hull &lt;- sf::st_sf(hull)\n  hull$ID &lt;- psi[1]\n  list(hull = list(hull = hull))\n}\n\n# Define a function that apply the previous function to each PointSourceID from a LAS object\nflighline_polygons &lt;- function(las)\n{\n  u &lt;- las@data[ , enveloppes(X,Y, PointSourceID), by = PointSourceID]\n  hulls &lt;- Reduce(rbind, u$hull)\n  return(hulls)\n}\n\n# Test this function on a LAS\nhulls &lt;- flighline_polygons(las)\nplot(hulls, col = sf.colors(3, alpha = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n\n\n# It works so let make a function that works with a LAScatalog\nflighline_polygons &lt;- function(las)\n{\n  if (is(las, \"LAS\"))  {\n    u &lt;- las@data[ , enveloppes(X,Y, PointSourceID), by = PointSourceID]\n    hulls &lt;- Reduce(rbind, u$hull)\n    return(hulls)\n  }\n  \n  if (is(las, \"LAScluster\")) {\n    las &lt;- readLAS(las)\n    if (is.empty(las)) return(NULL)\n    hulls &lt;- flighline_polygons(las)\n    return(hulls)\n  }\n  \n  if (is(las, \"LAScatalog\")) {\n    opt_select(las) &lt;-  \"xyzp\"\n    options &lt;- list(\n      need_output_file = FALSE,\n      need_buffer = TRUE,\n      automerge = TRUE)\n    output &lt;- catalog_apply(las, flighline_polygons, .options = options)\n    hulls &lt;- dplyr::summarise(dplyr::group_by(output, ID), ID = ID[1])\n    return(hulls)\n  }\n  \n  stop(\"Invalid input\")\n}\n\nlibrary(future)\n#&gt; Warning: package 'future' was built under R version 4.2.3\nfuture::plan(multisession)\nopt_chunk_buffer(ctg) &lt;- 5\nopt_filter(ctg) &lt;- \"-drop_withheld -drop_z_below 0 -drop_z_above 40\"\nflightlines &lt;- flighline_polygons(ctg)\n\n\n\n\n\n\n\n\n#&gt; Chunk 1 of 25 (4%): state âœ“\n#&gt; Chunk 2 of 25 (8%): state âœ“\n#&gt; Chunk 3 of 25 (12%): state âœ“\n#&gt; Chunk 4 of 25 (16%): state âœ“\n#&gt; Chunk 5 of 25 (20%): state âœ“\n#&gt; Chunk 6 of 25 (24%): state âœ“\n#&gt; Chunk 7 of 25 (28%): state âœ“\n#&gt; Chunk 8 of 25 (32%): state âœ“\n#&gt; Chunk 9 of 25 (36%): state âœ“\n#&gt; Chunk 10 of 25 (40%): state âœ“\n#&gt; Chunk 11 of 25 (44%): state âœ“\n#&gt; Chunk 12 of 25 (48%): state âœ“\n#&gt; Chunk 13 of 25 (52%): state âœ“\n#&gt; Chunk 14 of 25 (56%): state âœ“\n#&gt; Chunk 15 of 25 (60%): state âœ“\n#&gt; Chunk 16 of 25 (64%): state âœ“\n#&gt; Chunk 17 of 25 (68%): state âœ“\n#&gt; Chunk 18 of 25 (72%): state âœ“\n#&gt; Chunk 19 of 25 (76%): state âœ“\n#&gt; Chunk 20 of 25 (80%): state âœ“\n#&gt; Chunk 21 of 25 (84%): state âœ“\n#&gt; Chunk 22 of 25 (88%): state âœ“\n#&gt; Chunk 25 of 25 (92%): state âœ“\n#&gt; Chunk 23 of 25 (96%): state âœ“\n#&gt; Chunk 24 of 25 (100%): state âœ“\nplot(flightlines, col = sf.colors(6, alpha = 0.5))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "",
    "text": "Warning: package 'rgl' was built under R version 4.2.3"
  },
  {
    "objectID": "index.html#people",
    "href": "index.html#people",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "People",
    "text": "People\nPresenter: Tristan Goodbody (UBC)\nAssistants:\n\nAlexandre Morin-Bernard (Laval)\nLeanna Stackhouse (UBC)\nLiam Irwin (UBC)"
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "Materials",
    "text": "Materials\nThis repository contains the material for a ~3 hour lidR tutorial workshop. You should install the material on your own machine from this repository. It contains the code, the shapefiles and point-clouds we will use. The workshop intends to:\n\nPresent an overview of what can be done with lidR\nGive users an understanding of how lidR may fit their needs\n\nFind the code, exercises, and solutions used in the .\\code sub-directory."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "Requirements",
    "text": "Requirements\n\nR version and Rstudio\n\nYou need to install a recent version of R i.e.Â R 4.0.x or newer.\nWe will work with Rstudio. This IDE is not mandatory to follow the workshop but is highly recommended.\n\n\n\nR Packages\nYou need to install the lidR package in its latest version (v &gt;= 4.0.0).\ninstall.packages(\"lidR\")\nTo run all code in the tutorial yourself, you will need to install the following packages. You can use lidR without them, however.\nlibs &lt;- c(\"geometry\",\"viridis\",\"future\",\"sf\",\"maptools\",\"terra\",\"mapview\",\"mapedit\",\"concaveman\")\n\ninstall.packages(libs)"
  },
  {
    "objectID": "index.html#estimated-schedule",
    "href": "index.html#estimated-schedule",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "Estimated schedule",
    "text": "Estimated schedule\n\nIntroduction and set-up (09:00)\nRead LAS and LAZ files (09:15)\nSpatial queries (09:35)\nArea-Based Approach (09:45)\nCanopy Height Model (10:00)\nDigital Terrain Model (10:10)\n\nâ€” Break until 10:30 â€”\n\nIndividual tree segmentation (10:30)\nFile collection processing engine (basic) (11:00)\nFile collection processing engine (advanced) (11:30)"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "Resources",
    "text": "Resources\nWe strongly recommend having the following resources available to you:\n\nThe lidR official documentation\nThe lidRbook of tutorials\n\nWhen working on exercises:\n\nStack Exchange with the lidR tag"
  },
  {
    "objectID": "index.html#lidr",
    "href": "index.html#lidr",
    "title": "lidR: (A workshop for) Airborne LiDAR Data Manipulation and Visualization for Forestry Applications",
    "section": "lidR",
    "text": "lidR\nlidR is an R package to work with LiDAR data developed at Laval University (QuÃ©bec). It was developed & continues to be maintained by Jean-Romain Roussel and was made possible between:\n\n2015 and 2018 thanks to the financial support of the AWARE project NSERC CRDPJ 462973-14; grantee Prof.Â Nicholas C. Coops.\n2018 and 2021 thanks to the financial support of the MinistÃ¨re des ForÃªts, de la Faune et des Parcs (QuÃ©bec).\n\nThe current release version of lidR can be found on CRAN and source code is hosted on GitHub."
  }
]